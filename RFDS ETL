import pandas as pd
import paramiko
import os
from datetime import datetime
import numpy as np
import sys # Added for more robust error handling in main block

# --- 1. Configuration (REPLACE MOCK VALUES) ---

# SFTP HOST 1 (QuickBase)
HOST1 = "sftp.host1.com"
PORT1 = 22
USER1 = "user1"
PASS1 = "password1"
REMOTE_PATH1_QB = "/remote/path/to/quickbase/"
FILE_NAME_QB = "QuickBase_RFDS_CD_TED.xlsx"

# SFTP HOST 2 (RFDS Status & Report, CD Report, TED Report)
HOST2 = "sftp.host2.com"
PORT2 = 22
USER2 = "user2"
PASS2 = "password2"
REMOTE_PATH2_STATUS = "/remote/path/to/rfds/status/"
FILE_PREFIX_STATUS = "RFDS_status_report_"
REMOTE_PATH2_REPORT = "/remote/path/to/rfds/report/"
FILE_PREFIX_REPORT = "RFDS_report_"
REMOTE_PATH2_CD = "/remote/path/to/cd/report/" # New CD Report Path
FILE_PREFIX_CD = "CD_report_"                  # New CD Report Prefix
REMOTE_PATH2_TED = "/remote/path/to/ted/report/" # New TED Report Path
FILE_PREFIX_TED = "TED_report_"                # New TED Report Prefix


# UPLOAD HOST (Where the final outputs go)
UPLOAD_HOST = "sftp.uploadhost.com"
UPLOAD_PORT = 22
UPLOAD_USER = "upload_user"
UPLOAD_PASS = "upload_password"
UPLOAD_REMOTE_PATH = "/remote/path/for/output/RFDS/" # Original RFDS Path
UPLOAD_REMOTE_PATH_CD = "/var/data/Support_Data/quickbase/CD/" # New CD Path
UPLOAD_REMOTE_PATH_TED = "/var/data/Support_Data/quickbase/TED/" # New TED Path

# Local Paths and File Settings
LOCAL_BASE_PATH = os.path.join(os.getcwd(), 'sftp_files')
FILE_EXTENSION = ".csv"

# --- 2. SFTP Helper Functions ---

def get_latest_file(sftp, remote_file_path, file_prefix, file_extension):
    """Get the name of the latest file from the SFTP server based on file prefix and modified time."""
    files = sftp.listdir_attr(remote_file_path)
    latest_file = None
    latest_timestamp = None 
    for fileattr in files:
        file = fileattr.filename
        if file.startswith(file_prefix) and file.endswith(file_extension):
            try:
                # Use st_mtime (modification time) which is an integer timestamp
                timestamp_obj = fileattr.st_mtime 
                
                if latest_timestamp is None or timestamp_obj > latest_timestamp:
                    latest_timestamp = timestamp_obj
                    latest_file = file
            except Exception:
                continue
    return latest_file

def sftp_connect_and_transfer(host, port, username, password, local_path, remote_path, filename=None, file_prefix=None, direction='download'):
    """Handles both SFTP connection and file transfer (download or upload)."""
    # Determine the directory path to create
    if direction == 'download':
        local_dir = local_path
    else: # upload
        local_dir = os.path.dirname(local_path)
        
    os.makedirs(local_dir, exist_ok=True)
        
    ssh = None
    sftp = None
    transfer_path = None
    
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, port=port, username=username, password=password, timeout=30)
        sftp = ssh.open_sftp()
        
        if direction == 'download':
            remote_filename = filename
            if file_prefix:
                remote_filename = get_latest_file(sftp, remote_path, file_prefix, FILE_EXTENSION)
            
            if not remote_filename:
                print(f"[WARN] No matching file found in {remote_path} with prefix/name {file_prefix or filename}.")
                return None
                
            full_remote_path = os.path.join(remote_path, remote_filename).replace('\\', '/')
            full_local_path = os.path.join(local_path, remote_filename)
            sftp.get(full_remote_path, full_local_path)
            transfer_path = full_local_path
            print(f"[SUCCESS] Downloaded: {full_remote_path} to {full_local_path}")
            
        elif direction == 'upload':
            remote_filename = os.path.basename(local_path)
            # Ensure the remote path exists before uploading (optional, but good practice)
            # This requires SFTP permissions to create directories
            try:
                sftp.stat(remote_path)
            except FileNotFoundError:
                print(f"[INFO] Remote path {remote_path} not found, attempting to create.")
                sftp.mkdir(remote_path)
                
            full_remote_path = os.path.join(remote_path, remote_filename).replace('\\', '/')
            sftp.put(local_path, full_remote_path)
            transfer_path = local_path
            print(f"[SUCCESS] Uploaded: {local_path} to {full_remote_path}")
            
    except Exception as e:
        print(f"[ERROR] SFTP {direction} error: {e}")
    finally:
        if sftp: sftp.close()
        if ssh: ssh.close()
        return transfer_path

def process_file_to_df(file_path, usecols=None):
    """Reads a file and returns its content as a DataFrame."""
    if not file_path or not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return pd.DataFrame()
    try:
        if file_path.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_path, engine='openpyxl', usecols=usecols)
        elif file_path.endswith('.csv'):
            df = pd.read_csv(file_path, usecols=usecols)
        else:
            print(f"Unsupported file format: {os.path.basename(file_path)}")
            return pd.DataFrame()
            
        print(f"[SUCCESS] Successfully read file: {os.path.basename(file_path)} ({len(df)} rows)")
        return df
    except Exception as e:
        print(f"Error reading file {os.path.basename(file_path)}: {e}")
        return pd.DataFrame()

# --- Reusable Data Standardization Helper ---

def standardize_site_col(df, col_name, root_id_col, df_name):
    """Standardizes Site # and creates the Join_Key."""
    site_col_present = col_name in df.columns
    root_col_present = root_id_col in df.columns
    
    if site_col_present:
        df[col_name] = (
            df[col_name].astype(str)
            .str.strip()
            .str.replace(r'\.0$', '', regex=True) 
            .str.zfill(4) 
        )
    else:
        print(f"Warning: Column '{col_name}' not found in {df_name}. Using empty string for key component.")
        df[col_name] = ""
    
    if not root_col_present:
        print(f"Warning: Column '{root_id_col}' not found in {df_name}. Using empty string for key component.")
        df[root_id_col] = "" 
        
    df['Join_Key'] = df[root_id_col].astype(str) + df[col_name].astype(str)
    
    return df


# --- 3. Main Download Logic ---

def download_rfds_files(local_path):
    """Downloads all required input files for the RFDS process."""
    print("--- Starting RFDS Input File Downloads ---")
    
    # File 1: QuickBase_RFDS_CD_TED.xlsx (HOST 1)
    path_qb = sftp_connect_and_transfer(
        HOST1, PORT1, USER1, PASS1, local_path, REMOTE_PATH1_QB, filename=FILE_NAME_QB, direction='download'
    )
    df_quickbase = process_file_to_df(path_qb)

    # File 2: RFDS_status_report_*.csv (HOST 2)
    path_status = sftp_connect_and_transfer(
        HOST2, PORT2, USER2, PASS2, local_path, REMOTE_PATH2_STATUS, file_prefix=FILE_PREFIX_STATUS, direction='download'
    )
    df_rfds_status = process_file_to_df(path_status)

    # File 3: RFDS_report_*.csv (HOST 2) - For CBRS (3.65) Site column
    path_report = sftp_connect_and_transfer(
        HOST2, PORT2, USER2, PASS2, local_path, REMOTE_PATH2_REPORT, file_prefix=FILE_PREFIX_REPORT, direction='download'
    )
    
    # Only read necessary columns for efficiency
    columns_to_read_report = ['Site - Project Name', 'Site #', 'CBRS (3.65) Site']
    df_rfds_report = process_file_to_df(path_report, usecols=columns_to_read_report)
    
    return df_quickbase, df_rfds_status, df_rfds_report


def download_cd_ted_reports(local_path):
    """Downloads the required input report files for the CD and TED processes."""
    print("\n--- Starting CD/TED Report Downloads ---")
    
    # File 1: CD_report_*.csv (HOST 2)
    path_cd_report = sftp_connect_and_transfer(
        HOST2, PORT2, USER2, PASS2, local_path, REMOTE_PATH2_CD, file_prefix=FILE_PREFIX_CD, direction='download'
    )
    df_cd_report = process_file_to_df(path_cd_report)
    
    # File 2: TED_report_*.csv (HOST 2)
    path_ted_report = sftp_connect_and_transfer(
        HOST2, PORT2, USER2, PASS2, local_path, REMOTE_PATH2_TED, file_prefix=FILE_PREFIX_TED, direction='download'
    )
    df_ted_report = process_file_to_df(path_ted_report)
    
    return df_cd_report, df_ted_report


# --- 4. RFDS Update Data Processing Function ---

def RFDS_Update(df_quickbase_orig, df_rfds_status, df_rfds_report, output_path):
    """Performs data manipulation, lookups, filtering, and saves/uploads the final RFDS Update file."""
    
    print("\n--- Starting RFDS Update Data Processing ---")
    df_quickbase = df_quickbase_orig.copy() # Use a copy

    if df_quickbase.empty or df_rfds_status.empty:
        print("Error: QuickBase or RFDS Status data is empty. Cannot proceed.")
        return pd.DataFrame()
        
    # Define common column names
    root_id_col = 'P:Viaero Root ID'
    site_col = 'Site #'
    status_qb_col = 'OV RFDS Review Status Text'
    status_status_col_base = 'RFDS Review Status Text'

    # --- 1. Filter df_quickbase ---
    column_to_filter = 'WPK:OV Work Package ID'
    df_quickbase = df_quickbase[~df_quickbase.get(column_to_filter, pd.Series()).astype(str).str.contains('Viaero Trial', na=False, case=False)].copy()
    print("Step 1: Filtered QuickBase to exclude 'Viaero Trial'.")

    # --- 2. Prepare for Merges (Create Consistent Join Key) ---
    df_quickbase = standardize_site_col(df_quickbase, site_col, root_id_col, 'QuickBase')
    df_rfds_status = standardize_site_col(df_rfds_status, site_col, root_id_col, 'RFDS Status')
    print("Step 2: Standardized 'Site #' and created 'Join_Key' in QuickBase and RFDS Status.")


    # --- 3. Handle Duplicates in df_rfds_status ---
    # Sort by 'Join_Key' and 'RFDS', keep the LAST (latest) occurrence
    if 'RFDS' in df_rfds_status.columns:
        df_rfds_status = df_rfds_status.sort_values(
            by=['Join_Key', 'RFDS'], ascending=[True, True], na_position='first'
        ).drop_duplicates(subset=['Join_Key'], keep='last').copy()
        print("Step 3: Handled duplicates in RFDS status file.")
    else:
        # If RFDS is not available, default to keeping the last row per Join_Key
        df_rfds_status = df_rfds_status.drop_duplicates(subset=['Join_Key'], keep='last').copy()
        print("Step 3: Handled duplicates in RFDS status file (no 'RFDS' column, kept last).")

    
    # --- 4. VLOOKUP (Merge df_quickbase with df_rfds_status) ---
    status_cols_to_bring = [
        status_status_col_base, 'Date Added', 'Approval Date', 'CTO Review Status',
        'Project Team Review Status', 'RF Engineer Review Status'
    ]
    
    existing_status_cols = [col for col in status_cols_to_bring if col in df_rfds_status.columns]
    
    status_rename_map = {col: f"{col}_RFDS_STATUS" for col in existing_status_cols}
    df_status_for_merge = df_rfds_status[['Join_Key'] + existing_status_cols].rename(columns=status_rename_map)
    
    df_quickbase = pd.merge(df_quickbase, df_status_for_merge, on='Join_Key', how='left')
    print("Step 4: Merged QuickBase with RFDS Status data.")
    
    # --- 5. Conditional Logic: IF/THEN/ELSE ---
    status_status_col_merged = f"{status_status_col_base}_RFDS_STATUS"
    
    if status_qb_col in df_quickbase.columns and status_status_col_merged in df_quickbase.columns:
        df_quickbase['Comparison_Result'] = np.where(
            # Standardize strings (strip, lower) for case-insensitive comparison
            df_quickbase[status_status_col_merged].astype(str).str.strip().str.lower() == 
            df_quickbase[status_qb_col].astype(str).str.strip().str.lower(),
            "OK",
            "Update"
        )
        print("Step 5: Applied 'OK'/'Update' comparison logic.")
    else:
        df_quickbase['Comparison_Result'] = "OK"
        missing_cols = []
        if status_qb_col not in df_quickbase.columns: missing_cols.append(status_qb_col)
        if status_status_col_merged not in df_quickbase.columns: missing_cols.append(status_status_col_merged)
        print(f"ðŸ›‘ Warning: Missing comparison columns: {', '.join(missing_cols)}. Defaulting to 'OK'.")
    
    # --- 6. VLOOKUP for "CBRS (3.65)Site" from df_rfds_report ---
    if not df_rfds_report.empty and 'Site - Project Name' in df_rfds_report.columns and site_col in df_rfds_report.columns:
        
        # Standardize and pad the Site # for the report file before creating the key
        df_rfds_report = standardize_site_col(df_rfds_report, site_col, 'Site - Project Name', 'RFDS Report')
        df_rfds_report.rename(columns={'Site - Project Name': root_id_col}, inplace=True) # Rename root ID for consistent Join_Key

        df_quickbase = pd.merge(
            df_quickbase,
            df_rfds_report[['Join_Key', 'CBRS (3.65) Site']],
            on='Join_Key',
            how='left'
        )
        print("Step 6: VLOOKUP done to bring 'CBRS (3.65) Site'.")
    else:
        df_quickbase['CBRS (3.65) Site'] = np.nan
        print("Warning: RFDS report file is empty or missing key columns. Skipping CBRS lookup.")


    # --- 7. Final Filter ---
    df_output_filtered = df_quickbase[df_quickbase['Comparison_Result'] == 'Update'].copy()
    print(f"\nFinal Filter applied. Kept {len(df_output_filtered)} rows with 'Update' status.")

    if df_output_filtered.empty:
        return pd.DataFrame()
        
    # --- 8. Output File Mapping ---
    mapping = {
        'P:Project ID': 'Project ID',
        'CBRS (3.65) Site': 'P_QB_CBRS_365_SITE',
        'P:[QB] Conditional Tower Top': 'P_QB_CONDITIONAL_TOWER_TOP',
        'P:[QB] RFDS Vendor Notes': 'P_QB_RFDS_VENDOR_NOTES',
        'P:[QB] RFDS Approval History': 'P_QB_RFDS_APPROVAL_HISTORY',
        'P:[QB] RFDS Redline Notes': 'P_QB_RFDS_REDLINE_NOTES',
        'P:[QB] RFDS Rejection Notes': 'P_QB_RFDS_REJECTION_NOTES',
        status_status_col_merged: 'P_QB_RFDS_REVIEW_STATUS_TEXT', 
        'Date Added_RFDS_STATUS': 'P_QB_DATE_ADDED',
        'Approval Date_RFDS_STATUS': 'P_QB_RFDS_DATE_APPROVED',
        'CTO Review Status_RFDS_STATUS': 'P_QB_CTO_REVIEW_STATUS',
        'Project Team Review Status_RFDS_STATUS': 'P_QB_PROJECT_TEAM_REVIEW_STATUS',
        'RF Engineer Review Status_RFDS_STATUS': 'P_QB_RF_ENGINEER_REVIEW_STATUS'
    }

    final_output_cols = [col for col in mapping.keys() if col in df_output_filtered.columns]
    df_output = df_output_filtered[final_output_cols].rename(columns=mapping)
    print("Step 8: Applied final column mapping.")

    
    # --- 9. Create local file named QB_RFDS_yyyymmdd ---
    current_date = datetime.now().strftime("%Y%m%d")
    file_name = f"QB_RFDS_{current_date}.csv"
    local_full_path = os.path.join(output_path, file_name)

    df_output.to_csv(local_full_path, index=False)
    print(f"\nStep 9: Output file created locally: {local_full_path}")


    # --- 10. Update this file in the SFTP path (UPLOAD HOST) ---
    sftp_connect_and_transfer(
        UPLOAD_HOST, UPLOAD_PORT, UPLOAD_USER, UPLOAD_PASS, local_full_path, UPLOAD_REMOTE_PATH, direction='upload'
    )
        
    return df_output


# --- 4.1 CD Update Data Processing Function ---

def CD_Update(df_quickbase_orig, df_cd_report, output_path):
    """
    Performs data manipulation for Construction Drawings (CD), merging QuickBase
    with the latest CD Report, filtering for 'Update', and uploading.
    """
    
    print("\n--- Starting CD Update Data Processing ---")
    df_quickbase = df_quickbase_orig.copy() # Use a copy
    
    if df_quickbase.empty or df_cd_report.empty:
        print("Error: QuickBase or CD Report data is empty. Cannot proceed.")
        return pd.DataFrame()
        
    # Define common column names
    root_id_col = 'P:Viaero Root ID'
    site_col = 'Site #'
    status_qb_col = 'P:[QB] - Preliminary or Final (CD)' 
    status_report_col_base = 'Construction Drawing Status Text'

    # --- 1. Filter df_quickbase ---
    column_to_filter = 'WPK:OV Work Package ID'
    df_quickbase = df_quickbase[~df_quickbase.get(column_to_filter, pd.Series()).astype(str).str.contains('Viaero Trial', na=False, case=False)].copy()
    print("Step 1: Filtered QuickBase to exclude 'Viaero Trial'.")

    # --- 2. Prepare for Merges (Create Consistent Join Key) ---
    df_quickbase = standardize_site_col(df_quickbase, site_col, root_id_col, 'QuickBase')
    df_cd_report = standardize_site_col(df_cd_report, site_col, root_id_col, 'CD Report')
    print("Step 2: Standardized 'Site #' and created 'Join_Key' in QuickBase and CD Report.")
    
    # --- 3. Handle Duplicates in df_cd_report ---
    if 'Date Created' in df_cd_report.columns:
        df_cd_report = df_cd_report.sort_values(
            by=['Join_Key', 'Date Created'], ascending=[True, True], na_position='first'
        ).drop_duplicates(subset=['Join_Key'], keep='last').copy()
        print("Step 3: Handled duplicates in CD report file (based on 'Date Created').")
    else:
        df_cd_report = df_cd_report.drop_duplicates(subset=['Join_Key'], keep='last').copy()
        print("Step 3: Handled duplicates in CD report file (no 'Date Created' column, kept last).")

    
    # --- 4. VLOOKUP (Merge df_quickbase with df_cd_report) ---
    cd_cols_to_bring = [
        status_report_col_base, 'Date Created', 'Final Approval Date'
    ]
    
    existing_cd_cols = [col for col in cd_cols_to_bring if col in df_cd_report.columns]
    
    cd_rename_map = {col: f"{col}_CD_REPORT" for col in existing_cd_cols}
    df_cd_for_merge = df_cd_report[['Join_Key'] + existing_cd_cols].rename(columns=cd_rename_map)
    
    df_quickbase = pd.merge(df_quickbase, df_cd_for_merge, on='Join_Key', how='left')
    print("Step 4: Merged QuickBase with CD Report data.")
    
    # --- 5. Conditional Logic: IF/THEN/ELSE ---
    status_report_col_merged = f"{status_report_col_base}_CD_REPORT"
    
    if status_qb_col in df_quickbase.columns and status_report_col_merged in df_quickbase.columns:
        df_quickbase['Comparison_Result'] = np.where(
            df_quickbase[status_report_col_merged].astype(str).str.strip().str.lower() == 
            df_quickbase[status_qb_col].astype(str).str.strip().str.lower(),
            "OK",
            "Update"
        )
        print("Step 5: Applied 'OK'/'Update' comparison logic.")
    else:
        df_quickbase['Comparison_Result'] = "OK"
        print("ðŸ›‘ Warning: Missing comparison columns. Defaulting to 'OK'.")
    
    # --- 6. VLOOKUP for CBRS 3.65 (SKIP) ---
    print("Step 6: Skipping CBRS (3.65) Site VLOOKUP as requested.")


    # --- 7. Final Filter ---
    df_output_filtered = df_quickbase[df_quickbase['Comparison_Result'] == 'Update'].copy()
    print(f"\nFinal Filter applied. Kept {len(df_output_filtered)} rows with 'Update' status.")

    if df_output_filtered.empty:
        return pd.DataFrame()
        
    # --- 8. Output File Mapping ---
    mapping = {
        'P:Project ID': 'Project ID',
        'P:[QB] - Purpose of Site Plan (CD)': 'P_QB__PURPOSE_OF_SITE_PLAN',
        'P:[QB] - Preliminary or Final (CD)': 'P_QB__PRELIMINARY_OR_FINAL',
        'P:[QB] - Drawing Approval History (CD)': 'P_QB__DRAWING_APPROVAL_HISTORY',
        'P:[QB] - Redline Approval Notes (CD)': 'P_QB__REDLINE_APPRV_NOTES_CD',
        'P:[QB] - Document Rejection Notes (TED)': 'P_QB__DRAWING_REJ_NOTES_CD', # Mapped to CD REJECTION NOTES
        status_report_col_merged: 'P_QB__CONSTR_DRAWING_STATUS', 
        'Date Created_CD_REPORT': 'P_QB__DATE_CREATED_CD',
        'Final Approval Date_CD_REPORT': 'P_QB_FINAL_APPROVAL_DATE_CD'
    }

    final_output_cols = [col for col in mapping.keys() if col in df_output_filtered.columns]
    df_output = df_output_filtered[final_output_cols].rename(columns=mapping)
    print("Step 8: Applied final column mapping.")

    
    # --- 9. Create local file named QB_CD_yyyymmdd.csv ---
    current_date = datetime.now().strftime("%Y%m%d")
    file_name = f"QB_CD_{current_date}.csv"
    local_full_path = os.path.join(output_path, file_name)

    df_output.to_csv(local_full_path, index=False)
    print(f"\nStep 9: Output file created locally: {local_full_path}")


    # --- 10. Update this file in the SFTP path (CD UPLOAD PATH) ---
    sftp_connect_and_transfer(
        UPLOAD_HOST, UPLOAD_PORT, UPLOAD_USER, UPLOAD_PASS, local_full_path, UPLOAD_REMOTE_PATH_CD, direction='upload'
    )
        
    return df_output

# --- 4.2 TED Update Data Processing Function ---

def TED_Update(df_quickbase_orig, df_ted_report, output_path):
    """
    Performs data manipulation for Technical Engineering Documents (TED), merging QuickBase
    with the latest TED Report, filtering for 'Update', and uploading.
    """
    
    print("\n--- Starting TED Update Data Processing ---")
    df_quickbase = df_quickbase_orig.copy() # Use a copy
    
    if df_quickbase.empty or df_ted_report.empty:
        print("Error: QuickBase or TED Report data is empty. Cannot proceed.")
        return pd.DataFrame()
        
    # Define common column names
    root_id_col = 'P:Viaero Root ID'
    site_col = 'Site #'
    status_qb_col = 'Document Review Status Text'
    status_report_col_base = 'Document Review Status Text'

    # --- 1. Filter df_quickbase ---
    column_to_filter = 'WPK:OV Work Package ID'
    df_quickbase = df_quickbase[~df_quickbase.get(column_to_filter, pd.Series()).astype(str).str.contains('Viaero Trial', na=False, case=False)].copy()
    print("Step 1: Filtered QuickBase to exclude 'Viaero Trial'.")

    # --- 2. Prepare for Merges (Create Consistent Join Key) ---
    df_quickbase = standardize_site_col(df_quickbase, site_col, root_id_col, 'QuickBase')
    df_ted_report = standardize_site_col(df_ted_report, site_col, root_id_col, 'TED Report')
    print("Step 2: Standardized 'Site #' and created 'Join_Key' in QuickBase and TED Report.")

    # --- 3. Handle Duplicates in df_ted_report ---
    if 'Date Added' in df_ted_report.columns:
        df_ted_report = df_ted_report.sort_values(
            by=['Join_Key', 'Date Added'], ascending=[True, True], na_position='first'
        ).drop_duplicates(subset=['Join_Key'], keep='last').copy()
        print("Step 3: Handled duplicates in TED report file (based on 'Date Added').")
    else:
        df_ted_report = df_ted_report.drop_duplicates(subset=['Join_Key'], keep='last').copy()
        print("Step 3: Handled duplicates in TED report file (no 'Date Added' column, kept last).")
    
    # --- 4. VLOOKUP (Merge df_quickbase with df_ted_report) ---
    ted_cols_to_bring = [
        status_report_col_base, 'Date Added', 'Approval Date'
    ]
    
    existing_ted_cols = [col for col in ted_cols_to_bring if col in df_ted_report.columns]
    
    ted_rename_map = {col: f"{col}_TED_REPORT" for col in existing_ted_cols}
    df_ted_for_merge = df_ted_report[['Join_Key'] + existing_ted_cols].rename(columns=ted_rename_map)
    
    df_quickbase = pd.merge(df_quickbase, df_ted_for_merge, on='Join_Key', how='left')
    print("Step 4: Merged QuickBase with TED Report data.")
    
    # --- 5. Conditional Logic: IF/THEN/ELSE ---
    status_report_col_merged = f"{status_report_col_base}_TED_REPORT"
    
    if status_qb_col in df_quickbase.columns and status_report_col_merged in df_quickbase.columns:
        df_quickbase['Comparison_Result'] = np.where(
            df_quickbase[status_report_col_merged].astype(str).str.strip().str.lower() == 
            df_quickbase[status_qb_col].astype(str).str.strip().str.lower(),
            "OK",
            "Update"
        )
        print("Step 5: Applied 'OK'/'Update' comparison logic.")
    else:
        df_quickbase['Comparison_Result'] = "OK"
        print("ðŸ›‘ Warning: Missing comparison columns. Defaulting to 'OK'.")
    
    # --- 6. VLOOKUP for CBRS 3.65 (SKIP) ---
    print("Step 6: Skipping CBRS (3.65) Site VLOOKUP as requested.")


    # --- 7. Final Filter ---
    df_output_filtered = df_quickbase[df_quickbase['Comparison_Result'] == 'Update'].copy()
    print(f"\nFinal Filter applied. Kept {len(df_output_filtered)} rows with 'Update' status.")

    if df_output_filtered.empty:
        return pd.DataFrame()
        
    # --- 8. Output File Mapping ---
    mapping = {
        'P:Project ID': 'Project ID',
        'P:Type of Document (TED)': 'P_TYPE_OF_DOCUMENT_TED',
        status_report_col_merged: 'P_QB_DOC_REVIEW_STATUS_TEXT', 
        'P:[QB] - Document Redline Notes (TED)': 'P_QB_DOC_REDLINE_NOTES_TED',
        'P:[QB] - Document Rejection Notes (TED)': 'P_QB_DOCUMENT_REJECTION_NOTES',
        'Date Added_TED_REPORT': 'P_QB__DATE_ADDED_TED',
        'Approval Date_TED_REPORT': 'P_QB__DATE_APPROVED_TED'
    }

    final_output_cols = [col for col in mapping.keys() if col in df_output_filtered.columns]
    df_output = df_output_filtered[final_output_cols].rename(columns=mapping)
    print("Step 8: Applied final column mapping.")

    
    # --- 9. Create local file named QB_TED_yyyymmdd.csv ---
    current_date = datetime.now().strftime("%Y%m%d")
    file_name = f"QB_TED_{current_date}.csv"
    local_full_path = os.path.join(output_path, file_name)

    df_output.to_csv(local_full_path, index=False)
    print(f"\nStep 9: Output file created locally: {local_full_path}")


    # --- 10. Update this file in the SFTP path (TED UPLOAD PATH) ---
    sftp_connect_and_transfer(
        UPLOAD_HOST, UPLOAD_PORT, UPLOAD_USER, UPLOAD_PASS, local_full_path, UPLOAD_REMOTE_PATH_TED, direction='upload'
    )
        
    return df_output


# --- 5. Execution Block ---

if __name__ == "__main__":
    
    # Check if this is running in an environment where paramiko can be installed/used
    # This is a defensive check, remove if not needed
    if 'paramiko' not in sys.modules:
        print("Warning: paramiko module may not be imported or available. SFTP operations will likely fail.")
        # Continue execution, assuming the user will resolve the environment issue.

    # --- PART 1: Download Required Files ---
    print("--- Starting All Downloads ---")
    
    # RFDS-related files (includes the main QuickBase file)
    df_qb_orig, df_rfds_status, df_rfds_report = download_rfds_files(LOCAL_BASE_PATH)
    
    # CD and TED reports
    df_cd_report, df_ted_report = download_cd_ted_reports(LOCAL_BASE_PATH)
    
    if df_qb_orig.empty:
        print("\nFATAL ERROR: QuickBase file could not be loaded. Halting all processing.")
    else:
        # --- PART 2: RFDS Update Process ---
        print("\n\n=============== Starting RFDS Update Process ===============")
        final_df_rfds = RFDS_Update(df_qb_orig, df_rfds_status, df_rfds_report, LOCAL_BASE_PATH)
        print("=============== RFDS Update Process Complete ===============")

        # --- PART 3: CD Update Process ---
        print("\n\n================= Starting CD Update Process =================")
        final_df_cd = CD_Update(df_qb_orig, df_cd_report, LOCAL_BASE_PATH)
        print("================= CD Update Process Complete =================")

        # --- PART 4: TED Update Process ---
        print("\n\n================= Starting TED Update Process ================")
        final_df_ted = TED_Update(df_qb_orig, df_ted_report, LOCAL_BASE_PATH)
        print("================= TED Update Process Complete ================")

        print("\n--- All Processes Finished ---")
        print(f"RFDS Records Updated: {len(final_df_rfds) if not final_df_rfds.empty else 0}")
        print(f"CD Records Updated: {len(final_df_cd) if not final_df_cd.empty else 0}")
        print(f"TED Records Updated: {len(final_df_ted) if not final_df_ted.empty else 0}")
