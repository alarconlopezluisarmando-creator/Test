import pandas as pd
import numpy as np
import paramiko
import os
from datetime import datetime
import openpyxl # Needed for reading .xlsx files
from io import StringIO

# --- 1. SFTP CREDENTIALS AND CONFIGURATION ---

# 1.1 SFTP HOST 1: For QuickBase_RFDS_CD_TED.xlsx
HOST1 = 'xxx' # Replace with actual host
PORT1 = 22
USER1 = 'xx'  # Replace with actual user
PASS1 = 'xx'  # Replace with actual password
REMOTE_PATH1_QB = '/home/samsung_sea2/es/Rancomm/'
FILE_NAME_QB = 'QuickBase_RFDS_CD_TED.xlsx'

# 1.2 SFTP HOST 2: For RFDS_status_report_*.csv and RFDS_report_*.csv
# NOTE: Using the same host/port/user/pass for all /var/data/... files as implied
# by the original structure, but separating them logically.
HOST2 = 'xxx' # Replace with actual host (likely the one from the original snippet)
PORT2 = 22
USER2 = 'xxxx' # Replace with actual user
PASS2 = 'xxxx' # Replace with actual password
REMOTE_PATH2_STATUS = '/var/data/Support_Data/quickbase/RFDS_status/'
REMOTE_PATH2_REPORT = '/var/data/Support_Data/quickbase/RFDS_3days/'
FILE_PREFIX_STATUS = 'RFDS_status_report_'
FILE_PREFIX_REPORT = 'RFDS_report_'
FILE_EXTENSION = '.csv'

# 1.3 SFTP HOST 3: For the Final Upload
UPLOAD_HOST = '54.225.75.239'
UPLOAD_PORT = 22
UPLOAD_USER = 'XXXX'
UPLOAD_PASS = 'XXXX' # Replace with actual password
UPLOAD_REMOTE_PATH = '/home/samsung_sea2/es/Inbound/Rancomm/Quickbase_Reports/RFDS/'

# Local directory for downloaded and output files
LOCAL_BASE_PATH = './sftp_local_data' 


# --- 2. SFTP Helper Functions (Re-using/Refining your initial code) ---

def get_latest_file(sftp, remote_file_path, file_prefix, file_extension):
    """Get the name of the latest file from the SFTP server based on file prefix and modified time."""
    files = sftp.listdir_attr(remote_file_path)
    latest_file = None
    latest_timestamp = None # We'll use st_mtime for latest
    for fileattr in files:
        file = fileattr.filename
        if file.startswith(file_prefix) and file.endswith(file_extension):
            try:
                # Use st_mtime (modification time) which is an integer timestamp
                timestamp_obj = fileattr.st_mtime 
                
                if latest_timestamp is None or timestamp_obj > latest_timestamp:
                    latest_timestamp = timestamp_obj
                    latest_file = file
            except Exception:
                continue
    return latest_file

def sftp_connect_and_transfer(host, port, username, password, local_path, remote_path, filename=None, file_prefix=None, direction='download'):
    """Handles both SFTP connection and file transfer (download or upload)."""
    os.makedirs(local_path, exist_ok=True)
    ssh = None
    sftp = None
    transfer_path = None
    
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, port=port, username=username, password=password, timeout=30)
        sftp = ssh.open_sftp()
        
        if direction == 'download':
            if file_prefix:
                remote_filename = get_latest_file(sftp, remote_path, file_prefix, FILE_EXTENSION)
            elif filename:
                remote_filename = filename
            else:
                raise ValueError("Must provide filename or file_prefix for download.")

            if not remote_filename:
                print(f"No matching file found in {remote_path} with prefix {file_prefix}.")
                return None
                
            full_remote_path = os.path.join(remote_path, remote_filename).replace('\\', '/')
            full_local_path = os.path.join(local_path, remote_filename)
            sftp.get(full_remote_path, full_local_path)
            transfer_path = full_local_path
            print(f"✅ Downloaded: {full_remote_path} to {full_local_path}")
            
        elif direction == 'upload':
            full_remote_path = os.path.join(remote_path, os.path.basename(local_path)).replace('\\', '/')
            sftp.put(local_path, full_remote_path)
            transfer_path = local_path
            print(f"✅ Uploaded: {local_path} to {full_remote_path}")

    except Exception as e:
        print(f"❌ SFTP {direction} error: {e}")
    finally:
        if sftp: sftp.close()
        if ssh: ssh.close()
        return transfer_path

def process_file_to_df(file_path, usecols=None):
    """Reads a file and returns its content as a DataFrame."""
    if not file_path or not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return pd.DataFrame()
    try:
        if file_path.endswith('.xlsx'):
            df = pd.read_excel(file_path, engine='openpyxl', usecols=usecols)
        elif file_path.endswith('.csv'):
            df = pd.read_csv(file_path, usecols=usecols)
        print(f"✅ Successfully read file: {os.path.basename(file_path)}")
        return df
    except Exception as e:
        print(f"Error reading file {os.path.basename(file_path)}: {e}")
        return pd.DataFrame()


# --- 3. Main Download Logic ---

def download_all_input_files(local_path):
    """Downloads all three required input files from their respective SFTP hosts."""
    print("--- Starting Input File Downloads ---")
    
    # File 1: QuickBase_RFDS_CD_TED.xlsx (HOST 1)
    path_qb = sftp_connect_and_transfer(
        HOST1, PORT1, USER1, PASS1, local_path, REMOTE_PATH1_QB, filename=FILE_NAME_QB, direction='download'
    )
    df_quickbase = process_file_to_df(path_qb)

    # File 2: RFDS_status_report_*.csv (HOST 2)
    path_status = sftp_connect_and_transfer(
        HOST2, PORT2, USER2, PASS2, local_path, REMOTE_PATH2_STATUS, file_prefix=FILE_PREFIX_STATUS, direction='download'
    )
    df_rfds_status = process_file_to_df(path_status)

    # File 3: RFDS_report_*.csv (HOST 2) - For CBRS (3.65) Site column
    path_report = sftp_connect_and_transfer(
        HOST2, PORT2, USER2, PASS2, local_path, REMOTE_PATH2_REPORT, file_prefix=FILE_PREFIX_REPORT, direction='download'
    )
    
    # Only read necessary columns for efficiency
    columns_to_read = ['Site - Project Name', 'Site #', 'CBRS (3.65) Site']
    df_rfds_report = process_file_to_df(path_report, usecols=columns_to_read)
    
    return df_quickbase, df_rfds_status, df_rfds_report


# --- 4. RFDS Update Data Processing Function (From previous response, fully integrated) ---

def RFDS_Update(df_quickbase, df_rfds_status, df_rfds_report, output_path):
    """Performs data manipulation, lookups, filtering, and saves/uploads the final RFDS Update file."""
    
    print("\n--- Starting RFDS Update Data Processing ---")
    
    if df_quickbase.empty or df_rfds_status.empty:
        print("Error: QuickBase or RFDS Status data is empty. Cannot proceed.")
        return pd.DataFrame()
        
    # --- 1. Filter df_quickbase ---
    column_to_filter = 'WPK:OV Work Package ID'
    df_quickbase = df_quickbase[~df_quickbase[column_to_filter].astype(str).str.contains('Viaero Trial', na=False, case=False)].copy()
    print("Step 1: Filtered QuickBase to exclude 'Viaero Trial'.")

    # --- 2. Complete "site #" in df_rfds_status with "0" until 4 digits ---
    site_col = 'site #'
    df_rfds_status[site_col] = df_rfds_status[site_col].astype(str).str.strip().str.zfill(4)
    print("Step 2: Padded 'site #' in RFDS status file.")
    
    # --- Prepare for Merges (Create Join Key) ---
    df_quickbase['Join_Key'] = df_quickbase['P:Viaero Root ID'].astype(str) + df_quickbase['site #'].astype(str)
    df_rfds_status['Join_Key'] = df_rfds_status['P:Viaero Root ID'].astype(str) + df_rfds_status[site_col].astype(str)
    
    # --- 4. Handle Duplicates in df_rfds_status ---
    # Sort by 'Join_Key' and 'RFDS', keep the LAST (latest) occurrence
    df_rfds_status = df_rfds_status.sort_values(by=['Join_Key', 'RFDS'], ascending=[True, True]).drop_duplicates(subset=['Join_Key'], keep='last').copy()
    print("Step 4: Handled duplicates in RFDS status file.")
    
    # --- 3. VLOOKUP (Merge df_quickbase with df_rfds_status) ---
    status_cols_to_bring = [
        'RFDS Review Status Text', 'Date Added', 'Approval Date', 'CTO Review Status', 
        'Project Team Review Status', 'RF Engineer Review Status'
    ]
    
    status_rename_map = {col: f"{col}_RFDS_STATUS" for col in status_cols_to_bring}
    df_status_for_merge = df_rfds_status[['Join_Key'] + status_cols_to_bring].rename(columns=status_rename_map)

    df_quickbase = pd.merge(df_quickbase, df_status_for_merge, on='Join_Key', how='left')
    print("Step 3: Merged QuickBase with RFDS Status data.")

    # --- 5. Conditional Logic: IF/THEN/ELSE ---
    status_qb_col = 'OV RFDS Review Status Text' 
    status_status_col = 'RFDS Review Status Text_RFDS_STATUS' 
    
    if status_qb_col in df_quickbase.columns and status_status_col in df_quickbase.columns:
        df_quickbase['Comparison_Result'] = np.where(
            df_quickbase[status_status_col].astype(str) == df_quickbase[status_qb_col].astype(str), 
            "OK", 
            "Update"
        )
        print("Step 5: Applied 'OK'/'Update' comparison logic.")
    else:
        df_quickbase['Comparison_Result'] = "OK" 
        print("Warning: Missing comparison columns. Defaulting to 'OK'.")
        
    
    # --- 6. VLOOKUP for "CBRS (3.65)Site" from df_rfds_report ---
    if not df_rfds_report.empty:
        df_rfds_report['Join_Key'] = df_rfds_report['Site - Project Name'].astype(str) + df_rfds_report['Site #'].astype(str).str.strip().str.zfill(4)
        
        df_quickbase = pd.merge(
            df_quickbase,
            df_rfds_report[['Join_Key', 'CBRS (3.65) Site']],
            on='Join_Key',
            how='left'
        )
        print("Step 6: VLOOKUP done to bring 'CBRS (3.65) Site'.")
    else:
        df_quickbase['CBRS (3.65) Site'] = np.nan
        print("Warning: RFDS report file is empty. Skipping CBRS lookup.")


    # --- 1. Final Filter: Only rows that have the filter "update" ---
    df_output_filtered = df_quickbase[df_quickbase['Comparison_Result'] == 'Update'].copy()
    print(f"\nFinal Filter applied. Kept {len(df_output_filtered)} rows with 'Update' status.")

    if df_output_filtered.empty:
        return pd.DataFrame()
        
    # --- 10. Output File Mapping ---
    mapping = {
        'P:Project ID': 'Project ID',
        'CBRS (3.65) Site': 'P_QB_CBRS_365_SITE',
        'P:[QB] Conditional Tower Top': 'P_QB_CONDITIONAL_TOWER_TOP',
        'P:[QB] RFDS Vendor Notes': 'P_QB_RFDS_VENDOR_NOTES',
        'P:[QB] RFDS Approval History': 'P_QB_RFDS_APPROVAL_HISTORY',
        'P:[QB] RFDS Redline Notes': 'P_QB_RFDS_REDLINE_NOTES',
        'P:[QB] RFDS Rejection Notes': 'P_QB_RFDS_REJECTION_NOTES',
        status_status_col: 'P_QB_RFDS_REVIEW_STATUS_TEXT', 
        'Date Added_RFDS_STATUS': 'P_QB_DATE_ADDED',
        'Approval Date_RFDS_STATUS': 'P_QB_RFDS_DATE_APPROVED',
        'CTO Review Status_RFDS_STATUS': 'P_QB_CTO_REVIEW_STATUS',
        'Project Team Review Status_RFDS_STATUS': 'P_QB_PROJECT_TEAM_REVIEW_STATUS',
        'RF Engineer Review Status_RFDS_STATUS': 'P_QB_PROJECT_TEAM_REVIEW_STATUS' # Mapped to Project Team Status (assuming a typo or simplified need)
    }

    final_output_cols = [col for col in mapping.keys() if col in df_output_filtered.columns]
    df_output = df_output_filtered[final_output_cols].rename(columns=mapping)
    print("Step 10: Applied final column mapping.")

    
    # --- 7. Create local file named QB_RFDS_yyyymmdd ---
    os.makedirs(output_path, exist_ok=True)
    current_date = datetime.now().strftime("%Y%m%d")
    file_name = f"QB_RFDS_{current_date}.csv"
    local_full_path = os.path.join(output_path, file_name)

    df_output.to_csv(local_full_path, index=False)
    print(f"\nStep 7: Output file created locally: {local_full_path}")


    # --- 8. Update this file in the SFTP path (UPLOAD HOST) ---
    sftp_connect_and_transfer(
        UPLOAD_HOST, UPLOAD_PORT, UPLOAD_USER, UPLOAD_PASS, local_full_path, UPLOAD_REMOTE_PATH, direction='upload'
    )
        
    return df_output


# --- 5. Execution Block ---

# 1. Download all required files
df_qb, df_status, df_report = download_all_input_files(LOCAL_BASE_PATH)

# 2. Run the processing and upload
final_df = RFDS_Update(df_qb, df_status, df_report, LOCAL_BASE_PATH)

print("\n--- Process Complete ---")
if not final_df.empty:
    print(f"Final records processed and uploaded: {len(final_df)} rows.")
    # Optional: Display the first few rows of the final output
    # print(final_df.head())
else:
    print("No 'Update' records were generated. No file was created or uploaded.")
