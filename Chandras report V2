import pandas as pd
import pyodbc
from sqlalchemy import create_engine
import numpy as np
import os
import re

# --- Configuration Variables (Same as before) ---
server = '105.52.12.164,1433'
USERNAME = 'rfuseradmin'
PSSWD = 'Newtisgreat!'
DB = 'RANCOMM'
CDU30_TYPE = 'macro_indoor_dist'
UADPF_TYPE = 'udu_cnf'
NSB_TYPE = 'nsb_type' 
NE_TYPES_FILTER = [CDU30_TYPE, UADPF_TYPE, NSB_TYPE]
OUTPUT_FILE = r'C:\Users\l5.lopez\Downloads\00_to delete\raw_data.csv'
CHUNK_SIZE = 50000 
PROJECT_TYPE_MAPPER = {
    1: 'MACRO', 2: 'Small Cell', 3: 'In Building', 4: 'DAS',
}
# ------------------------------------------------------------------

# --- Database Connection and Query Setup (Same as before) ---
server_tcp = 'tcp:' + server 
connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server_tcp};DATABASE={DB};Encrypt=yes;TrustServerCertificate=yes;UID={USERNAME};PWD={PSSWD}'
engine = create_engine(f'mssql+pyodbc:///?odbc_connect={connection_string}', fast_executemany=True)

MAIN_DATA_QUERY = f"""
SELECT M.[market_id], M.[NE_ID], M.[ne_type], M.[NE NAME], M.[region]
FROM [RANCOMM].[dbo].[daily_sites_mapping] M
WHERE M.ne_type IN ('{CDU30_TYPE}', '{UADPF_TYPE}', '{NSB_TYPE}')
"""
OV_LOOKUP_QUERY = """
SELECT [P_ENB_ID], [P:Project Site Type]
FROM [RANCOMM].[dbo].[OVData]
"""
# --------------------------------------------------------------------------

# --- 3. Memory-Efficient Processing ---

if os.path.exists(OUTPUT_FILE):
    os.remove(OUTPUT_FILE)
    print(f"Removed existing file: {OUTPUT_FILE}")

header_written = False
sc_conditions = ['sc_od', 'sc_ib', 'das_od', 'das_ib', 'cran_das', 'ib', 'das', 'sc', 'odas']

print(f"Starting memory-efficient process. Output: {OUTPUT_FILE}")
print(f"1. Fetching small OVData lookup table...")

# Load the small OVData lookup table ONCE
try:
    df_ov_lookup = pd.read_sql(OV_LOOKUP_QUERY, engine)
    print(f"   Successfully loaded {len(df_ov_lookup)} records for lookup.")
    
    # FIX 1: Rename the ID column for clarity
    df_ov_lookup.rename(columns={'P:Project Site Type': 'Project_Type_ID'}, inplace=True)
    
    # ðŸ’¡ ENHANCED CLEANING FOR LOOKUP KEY (P_ENB_ID)
    df_ov_lookup['P_ENB_ID'] = df_ov_lookup['P_ENB_ID'].astype(str).str.strip()
    df_ov_lookup['P_ENB_ID'] = df_ov_lookup['P_ENB_ID'].str.replace(r'\.0$', '', regex=True)
    # **NEW:** Aggressive cleaning to remove non-alphanumeric chars (e.g., tabs, weird spaces)
    df_ov_lookup['P_ENB_ID'] = df_ov_lookup['P_ENB_ID'].str.replace(r'[^\w\s]', '', regex=True).str.strip()
    
    # FIX FOR DUPLICATES: Keep only the first unique entry for each P_ENB_ID
    initial_lookup_count = len(df_ov_lookup)
    df_ov_lookup.drop_duplicates(subset=['P_ENB_ID'], keep='first', inplace=True)
    print(f"   Removed {initial_lookup_count - len(df_ov_lookup)} duplicate entries from lookup table. âœ…")

except Exception as e:
    print(f"FATAL ERROR: Could not load OVData lookup table. Verify column name [P_ENB_ID] and connection: {e}")
    exit()


print(f"2. Processing main sites data in chunks of {CHUNK_SIZE} rows...")

try:
    for chunk_df in pd.read_sql_query(MAIN_DATA_QUERY, engine, chunksize=CHUNK_SIZE):
        
        # --- Python Logic for Key Creation and Merge ---
        
        # 1. ENHANCED CLEANING: Prepare components for the Custom LTE key
        market_id_str = chunk_df['market_id'].astype(str).str.strip().str.replace(r'\.0$', '', regex=True)
        ne_id_str = chunk_df['NE_ID'].astype(str).str.strip().str.replace(r'\.0$', '', regex=True)
        
        # Create the Custom LTE key
        chunk_df['Custom LTE'] = market_id_str + ne_id_str.str.slice(-3)
        
        # **NEW:** Aggressive cleaning to Custom LTE key before merge
        chunk_df['Custom LTE'] = chunk_df['Custom LTE'].str.replace(r'[^\w\s]', '', regex=True).str.strip()
        
        # 2. PERFORM THE SINGLE-KEY MERGE 
        chunk_df = pd.merge(
            chunk_df, 
            df_ov_lookup, 
            left_on='Custom LTE',  # Main data key (now aggressively cleaned)
            right_on='P_ENB_ID',   # Lookup data key (now aggressively cleaned)
            how='left'
        )
        
        # 3. MAP THE NUMERIC ID TO DESCRIPTIVE TEXT
        chunk_df['Project Site Type'] = chunk_df['Project_Type_ID'].apply(
            lambda x: PROJECT_TYPE_MAPPER.get(x, 'Unknown/N/A') if pd.notna(x) else x
        )
        
        # Set any remaining NaN merge failures to a specific status, ensuring we don't accidentally fill unmapped IDs
        # We fill NaN first, then handle the 'Unknown/N/A' from the mapper.
        chunk_df['Project Site Type'] = chunk_df['Project Site Type'].fillna('Merge Key Mismatch')

        # Drop the intermediate ID column
        chunk_df.drop(columns=['Project_Type_ID'], inplace=True)
        
        # 4. Categorization (Macro/SC)
        chunk_df['NE NAME'] = chunk_df['NE NAME'].astype(str).str.strip().str.lower()
        chunk_df['category'] = chunk_df['NE NAME'].apply(
            lambda x: 'SC' if any(cond in x for cond in sc_conditions) or x.startswith('4') else 'Macro'
        )
        
        # 5. Add the S// ID
        chunk_df['S// ID'] = chunk_df['Custom LTE'] 
        
        # --- Write Chunk to CSV ---
        
        # Define the EXPLICIT list of columns for the output CSV
        output_columns = [
            'region', 'NE_ID', 'market_id', 'Custom LTE', 'S// ID', 'P_ENB_ID', 
            'ne_type', 'NE NAME', 'category', 'Project Site Type' 
        ]
        
        # Write header only for the first chunk
        chunk_df[output_columns].to_csv(
            OUTPUT_FILE, 
            mode='a', 
            header=not header_written, 
            index=False
        )
        
        if not header_written:
            print(f"   First chunk saved and header written.")
            header_written = True
        
    print("\nâœ… Data processing complete. All chunks saved to CSV.")

except Exception as e:
    print(f"\nFATAL ERROR during chunk processing: {e}")
