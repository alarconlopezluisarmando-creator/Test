import pandas as pd
import pyodbc
from sqlalchemy import create_engine
import numpy as np
import os
import re

# --- 0. Configuration Variables ---
# NOTE: Your connection details must be defined here.
server = '105.52.12.164,1433'
USERNAME = 'rfuseradmin'
PSSWD = 'Newtisgreat!'
DB = 'RANCOMM'

# Define the file path for the output
OUTPUT_FILE = r'C:\Users\l5.lopez\Downloads\00_to delete\final_ovdata_output.csv'
CHUNK_SIZE = 50000  # Number of rows to process in memory at a time

# --- MAPPING DICTIONARY (REQUIRED TO CONVERT ID TO NAME) ---
PROJECT_TYPE_MAPPER = {
    1: 'MACRO',
    2: 'Small Cell',
    3: 'In Building',
    4: 'DAS',
    # Ensure all possible IDs from OVData are mapped here
}
# ------------------------------------------------------------------

# --- 1. Database Connection and Query Setup ---

# Configure the connection string and engine
server_tcp = 'tcp:' + server 
connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server_tcp};DATABASE={DB};Encrypt=yes;TrustServerCertificate=yes;UID={USERNAME};PWD={PSSWD}'
engine = create_engine(f'mssql+pyodbc:///?odbc_connect={connection_string}', fast_executemany=True)

# --------------------------------------------------------------------------
# 2. SQL QUERIES 
# --------------------------------------------------------------------------

# 2a. Query for the new MAIN TABLE (OVData)
# This will be the LEFT side of the merge.
OV_MAIN_QUERY = """
SELECT 
    [P_ENB_ID],             
    [P:Project Site Type]
FROM [RANCOMM].[dbo].[OVData]
"""

# 2b. Query for the new LOOKUP TABLE (daily_sites_mapping) 
# This will be the RIGHT side of the merge.
# We fetch the fields needed to construct the merge key and the output columns.
LOOKUP_MAP_QUERY = """
SELECT
    M.[market_id],
    M.[NE_ID],
    M.[ne_type],
    M.[NE NAME],
    M.[region]
FROM [RANCOMM].[dbo].[daily_sites_mapping] M
"""
# --------------------------------------------------------------------------
# 3. Memory-Efficient Processing
# --------------------------------------------------------------------------

if os.path.exists(OUTPUT_FILE):
    os.remove(OUTPUT_FILE)
    print(f"Removed existing file: {OUTPUT_FILE}")

header_written = False
sc_conditions = [
    'sc_od', 'sc_ib', 'das_od', 'das_ib', 'cran_das', 
    'ib', 'das', 'sc', 'odas' 
]

print(f"Starting memory-efficient process. Output: {OUTPUT_FILE}")
print(f"1. Fetching smaller daily_sites_mapping lookup table...")

# Load the daily_sites_mapping lookup table ONCE (Now the RIGHT table)
try:
    df_lookup = pd.read_sql(LOOKUP_MAP_QUERY, engine)
    print(f"   Successfully loaded {len(df_lookup)} records for lookup. ✅")
    
    # 1. Prepare key for lookup table (RIGHT)
    market_id_str = df_lookup['market_id'].astype(str).str.strip().str.replace(r'\.0$', '', regex=True)
    ne_id_str = df_lookup['NE_ID'].astype(str).str.strip().str.replace(r'\.0$', '', regex=True)
    df_lookup['Custom LTE'] = market_id_str + ne_id_str.str.slice(-3)
    
    # 2. CRITICAL FIX FOR DUPLICATES: De-duplicate the lookup table
    # This prevents the 1:many join that caused duplicates. Keep the first entry.
    initial_lookup_count = len(df_lookup)
    df_lookup.drop_duplicates(subset=['Custom LTE'], keep='first', inplace=True)
    print(f"   Removed {initial_lookup_count - len(df_lookup)} duplicate entries from lookup table. ✅")

except Exception as e:
    print(f"FATAL ERROR: Could not load daily_sites_mapping lookup table: {e}")
    exit()


print(f"2. Processing main OVData in chunks of {CHUNK_SIZE} rows...")

try:
    for chunk_df in pd.read_sql_query(OV_MAIN_QUERY, engine, chunksize=CHUNK_SIZE):
        
        # --- Python Logic for Key Creation and Merge ---
        
        # 1. CRITICAL CLEANING: Prepare key for main OVData table (LEFT)
        # Apply the same cleaning to the merge key on the left side
        chunk_df['P_ENB_ID_Clean'] = chunk_df['P_ENB_ID'].astype(str).str.strip().str.replace(r'\.0$', '', regex=True)
        
        # 2. PERFORM THE LEFT MERGE
        # Use a LEFT join to keep ALL rows from OVData (the left side)
        chunk_df = pd.merge(
            chunk_df, 
            df_lookup, 
            left_on='P_ENB_ID_Clean', # OVData key
            right_on='Custom LTE',    # daily_sites_mapping key
            how='left'                # Keep all rows from OVData
        )
        
        # 3. MAP THE NUMERIC ID TO DESCRIPTIVE TEXT
        
        # Rename the original 'P:Project Site Type' to the final column name
        chunk_df.rename(columns={'P:Project Site Type': 'Project_Type_ID'}, inplace=True)
        
        # Convert the numeric ID to descriptive text using the mapper
        chunk_df['Project Site Type'] = chunk_df['Project_Type_ID'].map(PROJECT_TYPE_MAPPER).fillna('UNMAPPED ID')
        
        # 4. Apply Categorization to the merged data (will be NaN if no match)
        # Note: We must convert the column to string first to handle NaN/nulls introduced by the merge
        chunk_df['NE NAME'] = chunk_df['NE NAME'].astype(str).str.strip().str.lower()
        
        # Use np.where to handle the NaN values introduced by the merge
        chunk_df['category'] = np.where(
            chunk_df['NE NAME'] == 'nan', # If the merge failed, NE NAME will be 'nan' (string) or NaN (float)
            '',                          # Set category to blank/empty string for failed merges
            chunk_df['NE NAME'].apply(
                lambda x: 'SC' if any(cond in x for cond in sc_conditions) or x.startswith('4') else 'Macro'
            )
        )
        
        # 5. Clean up intermediate/temporary columns
        chunk_df.drop(columns=['P_ENB_ID_Clean', 'Project_Type_ID'], errors='ignore', inplace=True)
        
        # --- Write Chunk to CSV ---
        
        # Define the EXPLICIT list of columns for the output CSV, ensuring order
        output_columns = [
            'region',
            'NE_ID',
            'market_id', 
            'Custom LTE',       # The key from the right table
            'P_ENB_ID',         # The original key from OVData
            'ne_type',
            'NE NAME',
            'category', 
            'Project Site Type' # The derived value
        ]
        
        # Write header only for the first chunk
        chunk_df[output_columns].to_csv(
            OUTPUT_FILE, 
            mode='a', 
            header=not header_written, 
            index=False,
            # Force missing values to be empty strings (blank) as requested
            na_rep='' 
        )
        
        if not header_written:
            print(f"   First chunk saved and header written.")
            header_written = True
        
    print("\n✅ Data processing complete. All chunks saved to CSV.")

except Exception as e:
    print(f"\nFATAL ERROR during chunk processing: {e}")
