import os
import glob
import pandas as pd
from datetime import datetime, timedelta
import paramiko
import numpy as np
import pendulum
from airflow import DAG
from airflow.operators.python import PythonOperator

# --- Configuration for SFTP Upload ---
SFTP_HOST = '54.225.75.239'
SFTP_PORT = 22
SFTP_USERNAME = 'xxx'  # Replace with actual username
SFTP_PASSWORD = 'xxx'  # Replace with actual password
SFTP_REMOTE_PATH = '/home/samsung_sea2/es/Inbound/Rancomm/Quickbase_Reports/E911/'

# --- Project ID Mapping Table ---
# Based on the mapping provided in the request
PROJECT_ID_MAP = {
    'V:(F 1115) Site Audit Walk Completion-Finish': 'PF1115',
    'V:(A 1115) Site Audit Walk Completion-Finish': 'AF1115',
    'V:(F 1120) Site Audit Walk Package Received-Finish': 'PF1120',
    'V:(A 1120) Site Audit Walk Package Received-Finish': 'AF1120',
    'V:(F 1135) Ground Lease Exhibit Received-Finish': 'PF1135',
    'V:(A 1135) Ground Lease Exhibit Received-Finish': 'AF1135',
    'V:(F 1125) Ground Cabling BOM Received-Finish': 'PF1125',
    'V:(A 1125) Ground Cabling BOM Received-Finish': 'AF1125',
    'V:(F 1285) Mount Analysis Received-Finish': 'PF1285',
    'V:(A 1285) Mount Analysis Received-Finish': 'AF1285',
    'V:(F 1292) MMOD Received-Finish': 'PF1292',
    'V:(A 1292) MMOD Received-Finish': 'AF1292',
    'V:(F 1142) 90% Construction Drawing Received-Finish': 'PF1142',
    'V:(A 1142) 90% Construction Drawing Received-Finish': 'AF1142',
    'V:(F 1127) Tower Cabling BOM Received-Finish': 'PF1127',
    'V:(A 1127) Tower Cabling BOM Received-Finish': 'AF1127',
    'V:(F 1281) Structural Received-Finish': 'PF1281',
    'V:(A 1281) Structural Received-Finish': 'AF1281',
    'V:(F 1146) 100% Construction Drawing Received-Finish': 'PF1146',
    'V:(A 1146) 100% Construction Drawing Received-Finish': 'AF1146',
    'V:(F 1143) Rework- 90% Construction Drawing Received-Finish': 'PF1143',
    'V:(A 1143) Rework- 90% Construction Drawing Received-Finish': 'AF1143',
}


def upload_file_to_sftp(local_path, host, port, username, password, remote_path):
    """Uploads a file to a remote SFTP server."""
    try:
        transport = paramiko.Transport((host, port))
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)
        
        # Ensure the remote directory exists (optional, but good practice)
        # sftp.stat(remote_path) 
        
        remote_file_path = os.path.join(remote_path, os.path.basename(local_path))
        print(f"Uploading {local_path} to {remote_file_path}...")
        sftp.put(local_path, remote_file_path)
        print(f"File uploaded successfully to {remote_file_path}")
        
        sftp.close()
        transport.close()
    except Exception as e:
        print(f"SFTP upload failed: {e}")
        # In a real Airflow environment, you might want to raise an exception here
        # raise e


def read_excel_file(path, file_pattern):
    """
    Reads the most recent Excel file matching the pattern, automatically 
    detecting the header row starting with 'Project ID'.
    """
    files = glob.glob(os.path.join(path, file_pattern))

    if not files:
        raise FileNotFoundError(f"No files found matching pattern '{file_pattern}' in directory '{path}'")

    most_recent_file = max(files, key=os.path.getmtime)
    print(f"Reading file: {most_recent_file}")

    excel_file = pd.read_excel(most_recent_file, header=None)

    # Find the row that contains 'Project ID'
    # Use str(x) to handle mixed types when checking for 'Project ID'
    header_row_index = excel_file.apply(lambda row: row.astype(str).str.contains('Project ID').any(), axis=1).idxmax()

    if excel_file.iloc[header_row_index].astype(str).str.contains('Project ID').any():
        # Create the DataFrame, skipping the rows before the header row
        df = excel_file.iloc[header_row_index:].reset_index(drop=True)

        # Set the first row as the header
        df.columns = df.iloc[0]
        df = df.iloc[1:].reset_index(drop=True)

        # Drop columns that are entirely NaN or entirely empty/whitespace strings
        df = df.dropna(axis=1, how='all')
        df = df.loc[:, ~(df.apply(lambda col: col.astype(str).str.strip().eq('').all()))]

        # Ensure all column names are strings for later merging
        df.columns = df.columns.astype(str)
        # Clean up column names by stripping whitespace
        df.columns = df.columns.str.strip()
        
        # Reset the index
        df = df.reset_index(drop=True)

        return df
    else:
        raise ValueError(f"Could not reliably find 'Project ID' header in file: {most_recent_file}")


def process_and_upload_reports(base_path):
    """
    Core function to read, process, compare, and upload the report files.
    """
    telamon_pattern = 'Telamon_report_*.xlsx'
    daily_ov_pattern = '44020 Daily OV Update*.xlsx'
    
    # 1. Read files
    try:
        # Assuming both files are in the same base_path, as only one path was provided
        # Use a more specific path if the files are in different folders
        df_telamon = read_excel_file(base_path, telamon_pattern)
        df_daily = read_excel_file(base_path, daily_ov_pattern)
    except FileNotFoundError as e:
        print(f"Skipping processing due to missing files: {e}")
        return # Stop execution if files aren't found
    except ValueError as e:
        print(f"Skipping processing due to header issue: {e}")
        return

    # Ensure key columns exist and are named correctly
    key_cols_telamon = ['Root ID', 'Other 1 - Site Number', 'Project ID']
    key_cols_daily = ['Project ID']

    if not all(col in df_telamon.columns for col in key_cols_telamon):
        print("Telamon report is missing one of the required columns: 'Root ID', 'Other 1 - Site Number', 'Project ID'")
        return

    if 'Project ID' not in df_daily.columns:
        print("Daily OV Update report is missing the required column: 'Project ID'")
        return

    # 2. Map Project ID in Telamon report
    # Create the lookup key for Telamon: 'Root ID' + 'Other 1 - Site Number' + 'Project ID'
    df_telamon['Lookup_Key'] = df_telamon['Project ID']
    
    # Apply the VLOOKUP/Mapping logic
    df_telamon['Mapped_Project ID'] = df_telamon['Lookup_Key'].map(PROJECT_ID_MAP).fillna(df_telamon['Project ID'])

    # Drop the temporary key column
    df_telamon.drop(columns=['Lookup_Key'], inplace=True)
    
    # Prepare dataframes for merge and comparison
    # Rename original columns to distinguish them after merge
    df_telamon.columns = [f'{col}_Telamon' if col != 'Mapped_Project ID' else col for col in df_telamon.columns]
    df_daily.columns = [f'{col}_Daily' if col != 'Project ID' else col for col in df_daily.columns]

    # Rename the mapped Project ID column to the merge key 'Project ID'
    df_telamon.rename(columns={'Mapped_Project ID': 'Project ID'}, inplace=True)
    df_daily.rename(columns={'Project ID': 'Project ID'}, inplace=True)

    # 3. Merge the two dataframes on the new 'Project ID' key
    # Use an outer merge to keep all records from both files
    merged_df = pd.merge(df_telamon, df_daily, on='Project ID', how='outer', suffixes=('_Telamon', '_Daily'))

    # Identify columns that are common (excluding 'Project ID' and the suffix columns)
    common_cols = [col.replace('_Telamon', '') for col in df_telamon.columns if col.endswith('_Telamon') and col != 'Project ID']
    
    # --- Comparison Logic ---
    
    # Initialize a column to track differences
    merged_df['Is_Different'] = False

    # Standardize empty/missing values to NaN for reliable comparison
    def clean_value(x):
        """Standardize various empty/missing representations to NaN."""
        if pd.isna(x) or str(x).strip() in ['', 'nan', 'NaT']:
            return np.nan
        return x

    for col in common_cols:
        col_tel = f'{col}_Telamon'
        col_daily = f'{col}_Daily'
        
        if col_tel in merged_df.columns and col_daily in merged_df.columns:
            # Clean and compare the columns. Use .apply(str) to avoid float/int comparison issues
            diff = merged_df.apply(lambda row: clean_value(row[col_tel]) != clean_value(row[col_daily]), axis=1, raw=False)
            merged_df['Is_Different'] = merged_df['Is_Different'] | diff

    # 4. Filter for rows where at least one column is different
    output_df = merged_df[merged_df['Is_Different']].copy()
    
    if output_df.empty:
        print("No differences found between the two reports. Skipping file creation and upload.")
        return

    # Select and reorder final columns for the output file
    final_cols = ['Project ID']
    
    # Add Telamon and Daily columns side-by-side for the final output
    for col in common_cols:
        col_tel = f'{col}_Telamon'
        col_daily = f'{col}_Daily'
        if col_tel in output_df.columns:
            final_cols.append(col_tel)
        if col_daily in output_df.columns:
            final_cols.append(col_daily)
            
    # Remove the comparison columns if they weren't part of the original common set
    output_df = output_df.filter(items=final_cols, axis=1)

    # 5. Add "Workplan Name" column
    output_df.insert(output_df.columns.get_loc('Project ID') + 1, "Workplan Name", "Viaero_Workplan")
    
    # Clean up column names for the final output
    new_cols = []
    for col in output_df.columns:
        if col.endswith('_Telamon'):
            new_cols.append(col.replace('_Telamon', '_Telamon')) # Keep original for Telamon
        elif col.endswith('_Daily'):
            new_cols.append(col.replace('_Daily', '_44020 Daily OV Update')) # Rename for Daily
        else:
            new_cols.append(col)
    
    output_df.columns = new_cols
    
    # 6. Save the file
    current_date = datetime.now().strftime('%Y%m%d')
    output_filename = f'TELAMON_IMPORT_{current_date}.xlsx'
    
    # Define a temporary local path for the file before SFTP upload
    output_path = os.path.join('/tmp', output_filename)
    
    output_df.to_excel(output_path, index=False)
    print(f"Output file created at: {output_path}")

    # 7. Upload the file via SFTP
    upload_file_to_sftp(
        output_path, 
        SFTP_HOST, 
        SFTP_PORT, 
        SFTP_USERNAME, 
        SFTP_PASSWORD, 
        SFTP_REMOTE_PATH
    )
    
    # Clean up the local file after upload
    os.remove(output_path)
    print(f"Local file {output_path} removed.")


# --- Airflow DAG Definition (using the user-provided structure) ---

# Define the base path for the report files
REPORT_PATH = '/var/data/Support_Data/quickbase/Telamon'

local_tz=pendulum.timezone("America/Chicago") # Central Time (CT)

default_args = {
    'owner': 'j.ferdous1',
    'start_date': datetime(2025, 10, 6),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'viaero_OV_ETL_final_rfds_upload_and_telamon_report',
    default_args=default_args,
    schedule_interval='20 8,10 * * *',  # run time
    catchup=False,
)

# --- Existing DAG Functions (copied from user request) ---
# ... (read_latest_rfds_status, download_file_from_sftp, upload_pdf_files_ssh)
# These are kept for completeness but not detailed here to avoid redundancy.
def read_latest_rfds_status(**kwargs):
    # ... (function body as provided by user)
    host = '105.52.12.194'
    port = 1022
    username = 'samrftool'
    password = 'nyHOPokaG3SP'

    remote_folder = '/var/data/Support_Data/quickbase/RFDS_status'
    local_folder = '/home/airflow/processed/quickbase/RFDS_status'

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(host, port=port, username=username, password=password)
    sftp = ssh.open_sftp()

    files = sftp.listdir(remote_folder)
    latest_file = max(files)
    remote_file_path = os.path.join(remote_folder, latest_file)
    local_file_path = os.path.join(local_folder, latest_file)
    sftp.get(remote_file_path, local_file_path)

    sftp.close()
    ssh.close()

    kwargs['ti'].xcom_push(key='rfds_status_path', value=local_file_path)


def download_file_from_sftp(**kwargs):
    # ... (function body as provided by user)
    ti = kwargs['ti']
    rfds_status_path = ti.xcom_pull(key='rfds_status_path')

    host = 'ftp.onevizion.com'
    username = 'sea2_es'
    password = 'M5v3WV3ThaG9'

    remote_file_path = '/home/samsung_sea2/rfds/Inbound/Archive'
    approved_status_path = '/home/airflow/processed/quickbase/RFDS_status'

    local_dir = '/home/airflow/processed/quickbase/viero_approved_rfds'

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(host, username=username, password=password)
    sftp = ssh.open_sftp()

    files = os.listdir(approved_status_path)
    latest_file = max(files, key=lambda x: os.path.getctime(os.path.join(approved_status_path, x)))
    latest_file_path = os.path.join(approved_status_path, latest_file)

    df = pd.read_csv(latest_file_path)
    approved_files = df[df['RFDS Review Status Text'] == 'Approved']

    status_file_path = os.path.join(local_dir, 'RFDS_status.xlsx')

    downloaded_files = []
    for index, row in approved_files.iterrows():
        file = row['RFDS']
        remote_file_path_with_name = os.path.join(remote_file_path, file)
        filename, file_extension = os.path.splitext(file)
        new_filename = filename.replace('RFDS', 'RFDS-APPROVED') + file_extension
        local_file_path = os.path.join(local_dir, new_filename)
        if not os.path.exists(local_file_path):
            try:
                sftp.get(remote_file_path_with_name, local_file_path)
                approved_files.loc[index, 'Download Status'] = 'Downloaded'
                downloaded_files.append(new_filename)
            except Exception as e:
                approved_files.loc[index, 'Download Status'] = 'Failed'
                print(f"Failed to download {file}: {e}")
        else:
            approved_files.loc[index, 'Download Status'] = 'Already Exist'

    approved_files.to_excel(status_file_path, index=False)

    sftp.close()
    ssh.close()

    kwargs['ti'].xcom_push(key='downloaded_files', value=downloaded_files)


def upload_pdf_files_ssh(**kwargs):
    # ... (function body as provided by user)
    ti = kwargs['ti']
    downloaded_files = ti.xcom_pull(key='downloaded_files')

    hostname = '54.225.75.239'
    username = 'sea2_es'
    password = 'M5v3WV3ThaG9'
    local_path = '/home/airflow/processed/quickbase/viero_approved_rfds'
    remote_path = '/home/samsung_sea2/rfds/Inbound/'

    ssh_client = paramiko.SSHClient()
    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh_client.connect(hostname=hostname, username=username, password=password)
    sftp_client = ssh_client.open_sftp()

    for filename in downloaded_files:
        if filename.endswith(".pdf"):
            local_file_path = os.path.join(local_path, filename)
            remote_file_path = os.path.join(remote_path, filename)
            sftp_client.put(local_file_path, remote_file_path)
            print(f"File uploaded successfully to {remote_file_path}")

    sftp_client.close()
    ssh_client.close()

# --- New Task for Telamon Report Processing ---

process_telamon_report_task = PythonOperator(
    task_id='process_and_upload_telamon_report',
    python_callable=process_and_upload_reports,
    op_kwargs={'base_path': REPORT_PATH},
    dag=dag,
)

# --- DAG Task Definitions (Original from user request) ---

read_latest_rfds_status_task = PythonOperator(
    task_id='read_latest_rfds_status',
    python_callable=read_latest_rfds_status,
    dag=dag,
)

download_file_from_sftp_task = PythonOperator(
    task_id='download_file_from_sftp',
    python_callable=download_file_from_sftp,
    dag=dag,
)

upload_pdf_files_ssh_task = PythonOperator(
    task_id='upload_pdf_files_ssh',
    python_callable=upload_pdf_files_ssh,
    dag=dag,
)

# --- DAG Task Dependencies ---

# The original RFDS pipeline
read_latest_rfds_status_task >> download_file_from_sftp_task >> upload_pdf_files_ssh_task

# The new Telamon processing task runs independently
process_telamon_report_task 
