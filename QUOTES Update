def upload_file_to_sftp(local_file_path, host, port, username, password, remote_file_path):
    ssh = None
    sftp = None
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, port=port, username=username, password=password)
        sftp = ssh.open_sftp()
        
        # Get the filename from the local file path
        filename = os.path.basename(local_file_path)
        
        # Upload the file
        full_remote_path = os.path.join(remote_file_path, filename)
        sftp.put(local_file_path, full_remote_path)
        print(f"File uploaded successfully to {full_remote_path}")
        
    except Exception as e:
        print(f"Error uploading file: {e}")
    finally:
        if sftp:
            sftp.close()
        if ssh:
            ssh.close()

def get_latest_file(sftp, remote_file_path, file_prefix):
    """
    Get the latest file from the SFTP server based on the file prefix.
    """
    files = sftp.listdir(remote_file_path)
    latest_file = None
    latest_timestamp = None
    for file in files:
        if file.startswith(file_prefix):
            try:
                file_stat = sftp.stat(os.path.join(remote_file_path, file))
                timestamp_obj = datetime.fromtimestamp(file_stat.st_mtime)
                
                if latest_timestamp is None or timestamp_obj > latest_timestamp:
                    latest_timestamp = timestamp_obj
                    latest_file = file
            except (ValueError, IndexError):
                continue
    return latest_file

def download_file_from_sftp_to_network(output_network_path, host, port, username, password, remote_file_path, file_prefix):
    """
    Downloads a single file from an SFTP server to a specified network directory.
    Returns the full local path of the downloaded file.
    """
    print(f"Starting SFTP file download for prefix: {file_prefix}")
    ssh = None
    sftp = None
    try:
        os.makedirs(output_network_path, exist_ok=True)

        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, port=port, username=username, password=password)
        sftp = ssh.open_sftp()

        latest_file = get_latest_file(sftp, remote_file_path, file_prefix)
        if latest_file:
            full_remote_path = os.path.join(remote_file_path, latest_file)
            full_local_path = os.path.join(output_network_path, latest_file)
            print(f"Downloading file: {full_remote_path} to {full_local_path}")
            try:
                sftp.get(full_remote_path, full_local_path)
                print(f"File downloaded successfully to {full_local_path}")
                return full_local_path
            except Exception as e:
                print(f"Error downloading file: {e}")
        else:
            print(f"No matching files found with prefix {file_prefix} in the specified SFTP directory.")
            return None

    except Exception as e:
        print(f"An error occurred during SFTP download: {e}")
        return None
    finally:
        if sftp:
            sftp.close()
        if ssh:
            ssh.close()

def process_and_compare_files(quote_report_path, quote_tracker_path):
    """
    Reads the two files, uses REGEX to cleanly extract Quote ID, creates a 
    SITE# + QUOTE_ID composite key, and compares the files.
    """
    print(f"\n--- Starting comparison between: {os.path.basename(quote_report_path)} and {os.path.basename(quote_tracker_path)} ---")
    
    # Define a regex pattern to reliably extract the core Quote ID (e.g., 'SV0282.00')
    # This pattern looks for two uppercase letters followed by digits and a decimal point.
    QUOTE_ID_PATTERN = r'([A-Z]{2}\d{4}\.\d{1,2})' 
    
    try:
        df_quotes = pd.read_csv(quote_report_path)
        df_tracker = pd.read_excel(quote_tracker_path, engine='openpyxl')
    except FileNotFoundError as e:
        print(f"Error: One of the downloaded files was not found at its local path. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names
    df_quotes.columns = df_quotes.columns.str.strip()
    df_tracker.columns = df_tracker.columns.str.strip()

    # --- Data Standardization for Merge ---

    # Process Quotes Report
    if 'Site #' in df_quotes.columns:
        df_quotes['Site #'] = df_quotes['Site #'].astype(str).str.strip().str.upper().str.zfill(4)
    else:
        print("Error: The required 'Site #' column was not found in the report.")
        return
    
    if 'Document' in df_quotes.columns:
        # Use str.extract with the regex pattern to reliably get only the core Quote ID
        df_quotes['QT:Quote ID'] = df_quotes['Document'].astype(str).str.extract(QUOTE_ID_PATTERN).fillna('')
        
        # Check if extraction was successful for any row
        if df_quotes['QT:Quote ID'].eq('').all():
             print("Error: Failed to extract a valid Quote ID from the 'Document' column using the regex pattern.")
             return
    else:
        print("Error: The required 'Document' column was not found in the report.")
        return
    
    if 'Review History' in df_quotes.columns:
        # 1. Clean the entire column first to remove any unexpected leading/trailing whitespace
        review_history_series = df_quotes['Review History'].astype(str).str.strip()
        
        # 2. Use a highly flexible regex to capture the date string.
        # We'll allow spaces around the brackets and be non-greedy in capturing the date parts.
        # Pattern explanation: Targets [3-letter month-1/2 digit day-2 digit year HH:MM AM/PM]
        # It explicitly handles the spaces and colon, but is flexible on the single/double digit day/hour/minute.
        DATE_REGEX = r'.*?\[\s*(\w{3}[-]\d{1,2}[-]\d{2}\s+\d{1,2}:\d{2}\s+[AP]M)\s*\].*'
    
        # Extract the DATE STRING (e.g., 'Nov-15-23 10:30 AM')
        # Use the 'str' accessor on the cleaned series.
        date_string_series = review_history_series.str.extract(DATE_REGEX, expand=False).str.strip()
    
        # 3. Convert the clean date string to a datetime object.
        # We keep the original format as it matches the pattern.
        df_quotes['Review History DT'] = pd.to_datetime(
            date_string_series,
            format='%b-%d-%y %I:%M %p',
            errors='coerce' # Set invalid parsing to NaT
        )
        
        # Check for successful extraction/conversion
        if df_quotes['Review History DT'].isna().all():
             # Provide a specific warning if conversion fails completely
             print("Error: Failed to extract a valid date/time from the 'Review History' column. Check the date format or the regex pattern.")
             return
    
        # 4. Sort by datetime and drop duplicates (using the extracted 'QT:Quote ID' as the subset)
        df_quotes = df_quotes.sort_values(by='Review History DT', ascending=False).drop_duplicates(
            subset='QT:Quote ID', 
            keep='first'
        )
        
        # 5. Drop the temporary datetime column
        df_quotes = df_quotes.drop(columns=['Review History DT'])
    else:
        print("Error: The required 'Review History' column was not found in the report.")
        return

    # 2. Process Quote Tracker
    if 'QT:Root Cell ID' in df_tracker.columns:
        df_tracker.rename(columns={'QT:Root Cell ID': 'Site #'}, inplace=True)
        df_tracker['Site #'] = df_tracker['Site #'].astype(str).str.strip().str.upper().str.zfill(4)
    else:
        print("Error: The required 'QT:Root Cell ID' was not found in the Tracker file.")
        return
    
    if 'QT:Quote ID' in df_tracker.columns:
        # Use str.extract with the regex pattern to reliably get only the core Quote ID
        df_tracker['QT:Quote ID'] = df_tracker['QT:Quote ID'].astype(str).str.extract(QUOTE_ID_PATTERN).fillna('')
    else:
        print("Error: The required 'QT:Quote ID' column was not found in the Tracker file.")
        return

    # --- CRITICAL FIX: Create Composite Merge Key ---
    # Merge will now happen on SITE# + QUOTE ID, ensuring a unique 1:1 match
    df_quotes['MERGE_KEY'] = df_quotes['Site #'] + '_' + df_quotes['QT:Quote ID']
    df_tracker['MERGE_KEY'] = df_tracker['Site #'] + '_' + df_tracker['QT:Quote ID']

    # --- Merge on the unique "MERGE_KEY" ---
    merged_df = pd.merge(
        df_quotes,
        df_tracker,
        on='MERGE_KEY', # Using the new composite key
        how='left',
        suffixes=('_quotes', '_tracker')
    )
    
    # ** Diagnostic Check **
    if merged_df['Site #_tracker'].isnull().all() and not df_quotes.empty:
        print("\n*** WARNING: ALL TRACKER COLUMNS ARE LIKELY BLANK (MERGE FAILURE) ***")
        print("This means NO COMPOSITE KEY matched between the Report and the Tracker.")
        print("Check Site # and Quote ID cleaning logic.")
        
    # --- Comparison Logic ---
    # Requirement 3: Concatenate key columns from Quotes_Report_
    merged_df['Conca_quotes'] = merged_df[[
        'Site #_quotes', 'Document Type - RAN', 'Review History',
        'Doc Review Status Text', 'Rejection Notes', 'Date Approved'
    ]].astype(str).agg('|'.join, axis=1)

    # Requirement 4: Concatenate key columns from Quote_Tracker_Data_
    merged_df['Conca_tracker'] = merged_df[[
        'Site #_tracker', 'QT:QB - Document Type', 'QT:QB - Review History',
        'QT:QB - Doc Review Status Text', 'QT:QB - Rejection Notes', 'QT:QB - Date Approved'
    ]].astype(str).agg('|'.join, axis=1)

    # Requirement 5: Compare and find differences
    # Include rows where the concatenated strings differ OR where the tracker data is entirely null (no match found)
    df_diff = merged_df[(merged_df['Conca_quotes'] != merged_df['Conca_tracker']) | merged_df['Site #_tracker'].isnull()]
    
    # Remove rows where the Report's Quote ID was invalid (blank)
    df_diff = df_diff[df_diff['QT:Quote ID_quotes'].notna() & (df_diff['QT:Quote ID_quotes'] != '')]

    # Remove duplicates and keep the latest based on QT_QB_REVIEW_HISTORY
    df_diff = df_diff.sort_values(by=['QT:Quote ID_quotes', 'QT:QB - Review History'], ascending=[True, False])
    df_diff = df_diff.drop_duplicates(subset='QT:Quote ID_quotes', keep='first')
    df_diff.to_excel(r'C:\Users\l5.lopez\Downloads\00_to delete\output.xlsx', index=False)

    # ====================================================================
    # === OUTPUT: DEBUG/COMPARISON FILE (Raw data of differences) ===
    # ====================================================================
    if not df_diff.empty:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        debug_file_name = f"DEBUG_Comparison_Differences_Raw_{timestamp}.xlsx"
        debug_output_path = os.path.join(os.path.dirname(quote_report_path), debug_file_name)
        
        # Include the MERGE_KEY in the debug file for easy validation
        df_diff.to_excel(debug_output_path, index=False)
        print(f"\n*** COMPARISON DEBUG FILE CREATED ***: Differences (merged raw data) saved to: {debug_output_path}")

    # ====================================================================
    # === OUTPUT: FINAL PROCESSED REPORT (Requirement 6) ===
    # ====================================================================
    if not df_diff.empty:
        df_output = df_diff[[
            # Use the clean, extracted QT:Quote ID from the quotes file (Source of Truth)
            'QT:Quote ID_quotes', 
            'Submittal Date',
            'Document Type - RAN',
            'Review History',
            'Doc Review Status Text',
            'Date Approved',
            'Rejection Notes',
            'Redline Notes'
        ]].rename(columns={
            # The output key is now guaranteed to be the clean, core Quote ID
            'QT:Quote ID_quotes': 'QUOTE_TRACKER_XITOR_KEY',
            'Submittal Date': 'QT_SUBMITTAL_DATE',
            'Document Type - RAN': 'QT_QB__DOCUMENT_TYPE',
            'Review History': 'QT_QB__REVIEW_HISTORY',
            'Doc Review Status Text': 'QT_QB__DOC_REVIEW_STATUS_TEXT',
            'Date Approved': 'QT_QB__DATE_APPROVED',
            'Rejection Notes': 'QT_QB__REJECTION_NOTES',
            'Redline Notes': 'QT_QB_RED_LINE_NOTES'
        })
        
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_QUOTE_{timestamp}.xlsx"
        output_path = os.path.join(os.path.dirname(quote_report_path), output_file_name)
        df_output.to_excel(output_path, index=False)
        print(f"Final Discrepancy Report saved to: {output_path}")
