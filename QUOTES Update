import paramiko
import os
import re
import pandas as pd
from datetime import datetime

# --- Configuration ---
EXCEL_EXTENSION = '.xlsx'
# Define a regex pattern to reliably extract the core Quote ID (e.g., 'SV0282.00')
# Adjust this if your actual Quote IDs follow a different format.
QUOTE_ID_PATTERN = r'([A-Z]{2}\d{4}\.\d{1,2})' 

# --- SFTP Helper Functions ---

def upload_file_to_sftp(local_file_path, host, port, username, password, remote_file_path):
    """Uploads a file to an SFTP server."""
    ssh = None
    sftp = None
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, port=port, username=username, password=password)
        sftp = ssh.open_sftp()
        
        # Get the filename from the local file path
        filename = os.path.basename(local_file_path)
        
        # Upload the file
        full_remote_path = os.path.join(remote_file_path, filename)
        sftp.put(local_file_path, full_remote_path)
        print(f"File uploaded successfully to {full_remote_path}")
        
    except Exception as e:
        print(f"Error uploading file: {e}")
    finally:
        if sftp:
            sftp.close()
        if ssh:
            ssh.close()

def get_latest_file(sftp, remote_file_path, file_prefix):
    """
    Gets the latest file from the SFTP server based on file prefix and dynamic date pattern.
    """
    
    # Define Date Pattern based on file prefix
    if 'Telamon_report_' in file_prefix:
        # Format: Telamon_report_YYYYMMDDHHMM_xxxx.xlsx
        DATE_PATTERN = r'(\d{12})'
        # Fallback to st_mtime if regex fails for this file
        use_timestamp_fallback = True
    elif '44020_Daily_OV_Update' in file_prefix:
        # Format: 44020_Daily_OV_Update-YYYY-MM-DD-HH-MM-SS
        DATE_PATTERN = r'(\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2})'
        # Fallback to st_mtime if regex fails for this file
        use_timestamp_fallback = True 
    else:
        # For other prefixes, rely on st_mtime as the only sort method
        DATE_PATTERN = None 
        use_timestamp_fallback = True

    files = sftp.listdir(remote_file_path)
    target_files = []

    for file in files:
        if file.startswith(file_prefix) and file.lower().endswith(EXCEL_EXTENSION):
            
            # --- Try sorting by Regex Timestamp first ---
            if DATE_PATTERN:
                match = re.search(DATE_PATTERN, file)
                if match:
                    # Use the extracted timestamp for sorting
                    target_files.append((file, match.group(1), None))
                    continue # Skip st_mtime if regex is successful
            
            # --- Fallback: Sort by modification time (st_mtime) ---
            if use_timestamp_fallback:
                try:
                    file_stat = sftp.stat(os.path.join(remote_file_path, file))
                    # The second element is the timestamp string, third is the modification time
                    target_files.append((file, None, file_stat.st_mtime))
                except Exception as e:
                    print(f"Could not stat file {file}: {e}")
                    continue

    if not target_files:
        print(f"No Excel files found matching prefix '{file_prefix}' in '{remote_file_path}'.")
        return None

    # Sort logic: 1. By regex date string (latest first). 2. By st_mtime (latest first)
    target_files.sort(key=lambda x: (x[1] if x[1] is not None else '', x[2] if x[2] is not None else 0), reverse=True)
    
    return target_files[0][0]

def download_file_from_sftp_to_network(output_network_path, host, port, username, password, remote_file_path, file_prefix):
    """
    Downloads the latest file from an SFTP server to a specified network directory.
    """
    print(f"Starting SFTP file download for prefix: {file_prefix}")
    ssh = None
    sftp = None
    try:
        os.makedirs(output_network_path, exist_ok=True)

        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, port=port, username=username, password=password)
        sftp = ssh.open_sftp()

        # Uses the updated function with dynamic date pattern and fallback
        latest_file = get_latest_file(sftp, remote_file_path, file_prefix)
        if latest_file:
            full_remote_path = os.path.join(remote_file_path, latest_file)
            full_local_path = os.path.join(output_network_path, latest_file)
            print(f"Downloading file: {full_remote_path} to {full_local_path}")
            try:
                sftp.get(full_remote_path, full_local_path)
                print(f"File downloaded successfully to {full_local_path}")
                return full_local_path
            except Exception as e:
                print(f"Error downloading file: {e}")
        else:
            print(f"No matching files found with prefix {file_prefix} in the specified SFTP directory.")
            return None

    except Exception as e:
        print(f"An error occurred during SFTP download: {e}")
        return None
    finally:
        if sftp:
            sftp.close()
        if ssh:
            ssh.close()

# --- Main Processing Logic ---

def process_and_compare_files(quote_report_path, quote_tracker_path):
    """
    Reads the two files, cleans and standardizes the data, and performs the comparison.
    """
    print(f"\n--- Starting comparison between: {os.path.basename(quote_report_path)} and {os.path.basename(quote_tracker_path)} ---")
    
    try:
        # Assuming the Quote Report is the 44020_Daily_OV_Update (xlsx) and the Tracker is Telamon (xlsx)
        # üéØ FIX: Read the Quote Report file with correct parameters (header=10, usecols="A:XFD")
        df_quotes = pd.read_excel(quote_report_path, header=10, usecols="A:XFD", engine='openpyxl')
        df_tracker = pd.read_excel(quote_tracker_path, engine='openpyxl')
    except FileNotFoundError as e:
        print(f"Error: One of the downloaded files was not found at its local path. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names
    df_quotes.columns = df_quotes.columns.str.strip()
    df_tracker.columns = df_tracker.columns.str.strip()
    
    # Check if the Quote Report file is empty due to incorrect header/column reading
    if df_quotes.empty:
        print("‚ö†Ô∏è ERROR: The Quote Report DataFrame is empty. Verify the `header` and `usecols` parameters in pd.read_excel().")
        return

    # --- Data Standardization for Merge ---

    # 1. Process Quotes Report (44020_Daily_OV_Update)
    if 'Other 1 - Site Number' in df_quotes.columns: # Changed from 'Site #' based on provided data
        df_quotes.rename(columns={'Other 1 - Site Number': 'Site #'}, inplace=True)
        df_quotes['Site #'] = df_quotes['Site #'].astype(str).str.strip().str.upper().str.zfill(4)
    else:
        print("Error: The required 'Other 1 - Site Number' column was not found in the report.")
        return
    
    if 'Document' in df_quotes.columns:
        df_quotes['QT:Quote ID'] = df_quotes['Document'].astype(str).str.extract(QUOTE_ID_PATTERN).fillna('')
        if df_quotes['QT:Quote ID'].eq('').all():
             print("Error: Failed to extract a valid Quote ID from the 'Document' column using the regex pattern.")
             return
    else:
        print("Error: The required 'Document' column was not found in the report.")
        return
    
    # üéØ FIX: Robust Review History Extraction (from previous fix)
    if 'Review History' in df_quotes.columns:
        review_history_series = df_quotes['Review History'].astype(str).str.strip()
        
        # FIXED REGEX: Allows flexible spacing before the AM/PM designation
        DATE_REGEX = r'.*?\[\s*(\w{3}[-]\d{1,2}[-]\d{2}\s+\d{1,2}:\d{2}\s*[AP]M)\s*\].*'
    
        date_string_series = review_history_series.str.extract(DATE_REGEX, expand=False).str.strip()
    
        df_quotes['Review History DT'] = pd.to_datetime(
            date_string_series,
            format='%b-%d-%y %I:%M %p',
            errors='coerce' 
        )
        
        if df_quotes['Review History DT'].isna().all():
             first_failing_value = date_string_series[df_quotes['Review History DT'].isna() & date_string_series.notna()].iloc[0] if not date_string_series[df_quotes['Review History DT'].isna() & date_string_series.notna()].empty else 'None Found'
             print(f"Error: Failed to extract a valid date/time from the 'Review History' column. Check the date format or the regex pattern.")
             print(f"First non-parsed value: '{first_failing_value}'")
             return
    
        df_quotes = df_quotes.sort_values(by='Review History DT', ascending=False).drop_duplicates(
            subset='QT:Quote ID', 
            keep='first'
        )
        
        df_quotes = df_quotes.drop(columns=['Review History DT'])
    else:
        print("Error: The required 'Review History' column was not found in the report.")
        return

    # 2. Process Quote Tracker (Telamon_report)
    if 'QT:Root Cell ID' in df_tracker.columns:
        df_tracker.rename(columns={'QT:Root Cell ID': 'Site #'}, inplace=True)
        df_tracker['Site #'] = df_tracker['Site #'].astype(str).str.strip().str.upper().str.zfill(4)
    else:
        print("Error: The required 'QT:Root Cell ID' was not found in the Tracker file.")
        return
    
    if 'QT:Quote ID' in df_tracker.columns:
        df_tracker['QT:Quote ID'] = df_tracker['QT:Quote ID'].astype(str).str.extract(QUOTE_ID_PATTERN).fillna('')
    else:
        print("Error: The required 'QT:Quote ID' column was not found in the Tracker file.")
        return

    # --- Merge and Comparison Logic ---
    
    # Create Composite Merge Key
    df_quotes['MERGE_KEY'] = df_quotes['Site #'] + '_' + df_quotes['QT:Quote ID']
    df_tracker['MERGE_KEY'] = df_tracker['Site #'] + '_' + df_tracker['QT:Quote ID']

    merged_df = pd.merge(
        df_quotes,
        df_tracker,
        on='MERGE_KEY', 
        how='left',
        suffixes=('_quotes', '_tracker')
    )
    
    # Diagnostic Check
    if merged_df['Site #_tracker'].isnull().all() and not df_quotes.empty:
        print("\n*** WARNING: ALL TRACKER COLUMNS ARE LIKELY BLANK (MERGE FAILURE) ***")
        
    # Requirement 3 & 4: Concatenate key columns
    merged_df['Conca_quotes'] = merged_df[[
        'Site #_quotes', 'Document Type - RAN', 'Review History',
        'Doc Review Status Text', 'Rejection Notes', 'Date Approved'
    ]].astype(str).agg('|'.join, axis=1)

    merged_df['Conca_tracker'] = merged_df[[
        'Site #_tracker', 'QT:QB - Document Type', 'QT:QB - Review History',
        'QT:QB - Doc Review Status Text', 'QT:QB - Rejection Notes', 'QT:QB - Date Approved'
    ]].astype(str).agg('|'.join, axis=1)

    # Requirement 5: Compare and find differences
    df_diff = merged_df[(merged_df['Conca_quotes'] != merged_df['Conca_tracker']) | merged_df['Site #_tracker'].isnull()]
    
    df_diff = df_diff[df_diff['QT:Quote ID_quotes'].notna() & (df_diff['QT:Quote ID_quotes'] != '')]

    # Remove duplicates and keep the latest
    df_diff = df_diff.sort_values(by=['QT:Quote ID_quotes', 'QT:QB - Review History'], ascending=[True, False])
    df_diff = df_diff.drop_duplicates(subset='QT:Quote ID_quotes', keep='first')
    
    # --- Output Files ---
    
    if not df_diff.empty:
        # DEBUG/COMPARISON FILE
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        debug_file_name = f"DEBUG_Comparison_Differences_Raw_{timestamp}.xlsx"
        # Assuming we save to the same directory as the input files
        debug_output_path = os.path.join(os.path.dirname(quote_report_path), debug_file_name) 
        df_diff.to_excel(debug_output_path, index=False)
        print(f"\n*** COMPARISON DEBUG FILE CREATED ***: Differences saved to: {debug_output_path}")

        # FINAL PROCESSED REPORT
        df_output = df_diff[[
            'QT:Quote ID_quotes', 
            'Submittal Date',
            'Document Type - RAN',
            'Review History',
            'Doc Review Status Text',
            'Date Approved',
            'Rejection Notes',
            'Redline Notes'
        ]].rename(columns={
            'QT:Quote ID_quotes': 'QUOTE_TRACKER_XITOR_KEY',
            'Submittal Date': 'QT_SUBMITTAL_DATE',
            'Document Type - RAN': 'QT_QB__DOCUMENT_TYPE',
            'Review History': 'QT_QB__REVIEW_HISTORY',
            'Doc Review Status Text': 'QT_QB__DOC_REVIEW_STATUS_TEXT',
            'Date Approved': 'QT_QB__DATE_APPROVED',
            'Rejection Notes': 'QT_QB__REJECTION_NOTES',
            'Redline Notes': 'QT_QB_RED_LINE_NOTES'
        })
        
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_QUOTE_{timestamp}.xlsx"
        output_path = os.path.join(os.path.dirname(quote_report_path), output_file_name)
        df_output.to_excel(output_path, index=False)
        print(f"Final Discrepancy Report saved to: {output_path}")

# --- Execution Block ---

def run_workflow():
    try:
        network_path = r'C:\Users\l5.lopez\Downloads\00_to delete'
        
        # --- SFTP Credentials and Paths ---
        # Note: You need to replace 'xxx' with actual credentials
        HOST1, PORT1, USER1, PASS1 = '54.225.75.239', 22, 'xxx', 'xxx'
        REMOTE_PATH1, PREFIX1 = '/home/samsung_sea2/es/Rancomm/', 'Telamon_report_'
        
        HOST2, PORT2, USER2, PASS2 = '105.52.12.194', 1022, 'xxx', 'xxx'
        REMOTE_PATH2, PREFIX2 = '/var/data/Support_Data/quickbase/Telamon', '44020_Daily_OV_Update'

        # Download Quote Tracker (Telamon_report_) - Should be the tracker file
        quote_tracker_path = download_file_from_sftp_to_network(network_path, HOST1, PORT1, USER1, PASS1, REMOTE_PATH1, PREFIX1)
        
        # Download Quote Report (44020_Daily_OV_Update) - Should be the report file
        quote_report_path = download_file_from_sftp_to_network(network_path, HOST2, PORT2, USER2, PASS2, REMOTE_PATH2, PREFIX2)
        
        # The logic below assumes: 
        #   quote_report_path = 44020_Daily_OV_Update (Needs header=10 fix)
        #   quote_tracker_path = Telamon_report_ (Normal read)
        
        if quote_report_path and quote_tracker_path:
            process_and_compare_files(quote_report_path, quote_tracker_path)
        else:
            print("One or both required files failed to download. Comparison aborted.")
    except Exception as e:
        print(f"An error occurred during file processing: {e}")

# Call the main function
run_workflow()
