import paramiko
import os
import re
import pandas as pd
from datetime import datetime
import glob

# Define the expected extension for the Excel file
EXCEL_EXTENSION = '.xlsx' 

def get_sftp_connection(host, port, username, password):
    """Establishes an SFTP connection and returns the transport and SFTP client."""
    transport = None
    sftp = None
    try:
        transport = paramiko.Transport((host, port))
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)
        return transport, sftp
    except Exception as e:
        print(f"Error connecting to SFTP at {host}:{port}: {e}")
        if transport:
            transport.close()
        return None, None

def close_sftp_connection(sftp, transport):
    """Closes the SFTP client and transport connection."""
    if sftp:
        sftp.close()
    if transport:
        transport.close()
    
def get_latest_remote_file(sftp, remote_file_path, file_prefix):
    """
    Lists files, finds the one matching the prefix/extension, 
    and returns the name of the latest file based on the timestamp.
    """
    
    if 'Telamon_report_' in file_prefix:
        DATE_PATTERN = r'(\d{12})'
    elif '44020 Daily OV Update-' in file_prefix:
        DATE_PATTERN = r'(\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2})'
    else:
        print(f"Error: Unknown file prefix '{file_prefix}'. Cannot determine date pattern.")
        return None
        
    try:
        files = sftp.listdir(remote_file_path)
        target_files = []
        for file in files:
            if file.startswith(file_prefix) and file.lower().endswith(EXCEL_EXTENSION):
                match = re.search(DATE_PATTERN, file)
                if match:
                    date_key = match.group(1) 
                    target_files.append((file, date_key))
        
        if not target_files:
            print(f"No Excel files found matching prefix '{file_prefix}' and extension '{EXCEL_EXTENSION}' in '{remote_file_path}'.")
            return None

        target_files.sort(key=lambda x: x[1], reverse=True)
        return target_files[0][0]
        
    except Exception as e:
        print(f"Error listing files in '{remote_file_path}': {e}")
        return None

def download_file_from_sftp_to_network(network_path, host, port, username, password, remote_file_path, file_prefix):
    """Connects, finds the latest file, downloads it, and closes the connection."""
    print(f"Attempting to connect to {host}:{port} for files starting with '{file_prefix}'...")
    
    transport, sftp = get_sftp_connection(host, port, username, password)
    
    if not sftp:
        return None

    try:
        latest_file_remote_name = get_latest_remote_file(sftp, remote_file_path, file_prefix)
        
        if not latest_file_remote_name:
            return None
        
        remote_full_path = os.path.join(remote_file_path, latest_file_remote_name)
        local_file_path = os.path.join(network_path, latest_file_remote_name)
        
        print(f"Latest remote file: {latest_file_remote_name}. Downloading to: {local_file_path}")
        
        sftp.get(remote_full_path, local_file_path)
        
        print(f"Download successful. File size on disk (bytes): {os.path.getsize(local_file_path)}")
        return local_file_path
        
    except Exception as e:
        print(f"An error occurred during file download for prefix '{file_prefix}': {e}")
        return None
    finally:
        close_sftp_connection(sftp, transport)

def upload_file_to_sftp(file_path, host, port, username, password, remote_path):
    """Uploads a local file to the remote SFTP path."""
    transport, sftp = None, None
    try:
        transport, sftp = get_sftp_connection(host, port, username, password)
        if sftp:
            file_name = os.path.basename(file_path)
            sftp.put(file_path, os.path.join(remote_path, file_name))
            print(f"Upload successful: {file_name} to {remote_path}")
    except Exception as e:
        print(f"An error occurred during file upload: {e}")
    finally:
        close_sftp_connection(sftp, transport)

def read_excel_file(path):
    """
    Reads the '44020 Daily OV Update' file, applying robust fixes for row/column offset 
    and cleaning column names/data to prevent indexing errors.
    """
    file_pattern = '44020 Daily OV Update*.xlsx'
    files = glob.glob(os.path.join(path, file_pattern))

    if not files:
        print(f"No files found matching pattern '{file_pattern}' in directory '{path}'")
        return pd.DataFrame()

    most_recent_file = max(files, key=os.path.getmtime)
    print(f"Processing local file: {os.path.basename(most_recent_file)}")
    
    try:
        # Fixed reading parameters: start at Row 12 (index 11), Column B
        df = pd.read_excel(
            most_recent_file, 
            skiprows=11, 
            header=0,     
            usecols="B:"  
        )
    except Exception as e:
        print(f"FATAL ERROR reading Excel file {os.path.basename(most_recent_file)} with custom parameters: {e}")
        return pd.DataFrame()
    
    # --- Column Name Cleaning and Filtering (MOST ROBUST) ---
    
    # 1. Convert names to string, strip whitespace
    df.columns = df.columns.astype(str).str.strip() 
    
    # 2. Filter out columns whose NAMES are empty, 'nan', or 'None'
    #    Uses list indexing which is robust against the 'multidimensional key' error.
    valid_cols = [col for col in df.columns if col and col.lower() not in ('nan', 'none')]
    df = df[valid_cols]
    
    # 3. Drop columns where ALL DATA is empty/NaN
    df = df.dropna(axis=1, how='all')

    # 4. Drop columns where ALL DATA is whitespace/empty string.
    #    This uses a simple Boolean mask on the pre-cleaned DataFrame.
    mask_only_whitespace = df.astype(str).apply(lambda col: col.str.strip().eq('').all())
    df = df.loc[:, ~mask_only_whitespace]
    
    # --- End Robust Method ---

    # Reset the index
    df = df.reset_index(drop=True)

    # Sanity check for the merge key
    if 'Other 1 - Site Number' not in df.columns:
        print(f"FINAL WARNING: 'Other 1 - Site Number' column not found after cleaning. Columns found: {df.columns.tolist()}")
        if df.empty:
            print("CRITICAL: DataFrame is empty. Check the data in the Excel file.")
            
    return df

def process_files(quotes_file_path, tracker_file_path, network_path):
    # Read the files
    print(f"Reading tracker file: {os.path.basename(tracker_file_path)}")
    telamon_df = pd.read_excel(tracker_file_path) 
    
    # Use the robust reader for the daily update file
    daily_update_df = read_excel_file(os.path.dirname(quotes_file_path))

    if daily_update_df.empty:
        print(" ABORT: The '44020 Daily OV Update' file is empty or failed to load correctly. Aborting comparison.")
        return 

    # Merge the DataFrames
    print("Merging dataframes...")
    merged_df = pd.merge(telamon_df, daily_update_df, 
                         left_on='P:Viaero Root ID', 
                         right_on='Other 1 - Site Number', 
                         suffixes=('_telamon', '_daily_update'), 
                         how='outer')

    # Identify differences
    diff_df = pd.DataFrame()
    columns_to_check = set(telamon_df.columns) & set(daily_update_df.columns)
    
    for column in columns_to_check:
        if column not in ['P:Viaero Root ID', 'Other 1 - Site Number']: 
            telamon_col = f'{column}_telamon'
            daily_col = f'{column}_daily_update'
            
            if telamon_col in merged_df.columns and daily_col in merged_df.columns:
                temp_df = merged_df[
                    merged_df[telamon_col].astype(str).fillna('').str.strip() != 
                    merged_df[daily_col].astype(str).fillna('').str.strip()
                ]
                diff_df = pd.concat([diff_df, temp_df])
                
    diff_df = diff_df.drop_duplicates(subset=['P:Viaero Root ID', 'Other 1 - Site Number']).copy() 

    # Create output DataFrame
    output_df = pd.DataFrame()
    output_df['Project ID'] = diff_df.get('Project ID_telamon', diff_df.get('Project ID', ''))
    output_df['Workplan Name'] = 'Viaero_Workplan'
    output_df['Mapped Project ID'] = output_df['Project ID'].astype(str).apply(lambda x: x.replace('V:(F', 'PF').replace('V:(A', 'AF'))

    for column in telamon_df.columns:
        telamon_col = column if column in ['Project ID'] else f'{column}_telamon'
        if telamon_col in diff_df.columns:
            output_df[column] = diff_df[telamon_col]
        
    output_df = output_df.dropna(how='all', subset=output_df.columns.difference(['Workplan Name'])).reset_index(drop=True)

    # Save output file and upload
    output_file_name = f'TELAMON_IMPORT_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv' 
    output_path = os.path.join(network_path, output_file_name)
    output_df.to_csv(output_path, index=False)
    print(f"Output file saved successfully: {output_path}")

    sftp_host = '54.225.75.239'
    sftp_port = 22
    sftp_username = 'sea2_es'
    sftp_password = 'M5v3WV3ThaG9'
    sftp_remote_path = '/home/samsung_sea2/es/Inbound/Rancomm/Telamon Reports/'
    print("Attempting to upload the final CSV...")
    upload_file_to_sftp(output_path, sftp_host, sftp_port, sftp_username, sftp_password, sftp_remote_path)

def download_and_process_files():
    try:
        # --- CONFIGURATION START ---
        # The local folder where the script saves/reads all files.
        network_path = r'C:\Users\l5.lopez\Downloads\00_to delete' 
        
        # --- LOCAL OVERRIDE FOR 44020 FILE ---
        # The script will look for '44020 Daily OV Update*.xlsx' in this folder
        local_44020_folder = network_path 
        # quotes_file_path is a placeholder to pass the folder path to process_files
        quotes_file_path = os.path.join(local_44020_folder, '44020 Daily OV Update-LOCAL_READ.xlsx')
        
        print("\n*** LOCAL OVERRIDE: Reading '44020 Daily OV Update-' from the local folder. ***")
        
        # SFTP 1: Telamon Tracker - MUST BE DOWNLOADED
        host1 = '54.225.75.239'
        port1 = 22
        username1 = 'sea2_es'
        password1 = 'M5v3WV3ThaG9'
        remote_file_path1 = '/home/samsung_sea2/es/Rancomm/'
        file_prefix1 = 'Telamon_report_'
        
        tracker_file_path = download_file_from_sftp_to_network(network_path, host1, port1, username1, password1, remote_file_path1, file_prefix1)
        
        # --- CONFIGURATION END ---

        if tracker_file_path:
            print(f"Quotes file path set to local folder: {os.path.dirname(quotes_file_path)}")
            process_files(quotes_file_path, tracker_file_path, network_path)
        else:
            print("Telamon tracker file failed to download. Comparison aborted.")
    except Exception as e:
        print(f"An error occurred during file processing: {e}")

# Execute the main function
download_and_process_files()
