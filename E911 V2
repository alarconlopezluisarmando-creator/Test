V2

import pandas as pd
import os
from datetime import datetime

# NOTE: The 'upload_file_to_sftp' function is defined here for completeness
# but will be commented out unless you provide the function's definition.
# If you need this function, you must define it separately in your script.
# def upload_file_to_sftp(...):
#     pass 

def process_e911_discrepancy_report(daily_file_path, report_file_path):
    """
    Compares E911_Daily_ (Old) and E911_Report_ (New) files, identifies 
    discrepancies based on key status columns, and outputs a report 
    with the required column mapping, using data from the Daily file.
    
    Args:
        daily_file_path (str): Path to the E911_Daily_ file (Left DF).
        report_file_path (str): Path to the E911_Report_ file (Right DF).
    """
    print(f"\n--- Starting comparison: {os.path.basename(daily_file_path)} (Old) vs. {os.path.basename(report_file_path)} (New) ---")
    
    # 1. Define Column Mapping and Keys
    COLUMN_MAP = {
        'P:Project ID': {'E911_Report_Col': 'Project ID', 'Output_Col': 'Project ID'},
        'P:FCC Site ID Number': {'E911_Report_Col': 'FCC Location ID', 'Output_Col': 'FCC Site ID'},
        'P:Viaero Root ID': {'E911_Report_Col': 'Site #', 'Output_Col': 'Viaero Root ID'},
        'P:Viaero Site Name': {'E911_Report_Col': 'Site Name', 'Output_Col': 'Viaero Site Name'},
        'P:TVW estimated ready/rough forecast date (QB)': {'E911_Report_Col': 'TVW estimated ready/rough forecast date:', 'Output_Col': 'P_TVW_EST_READY_FORE_QB'},
        'P:911 TVW - Blank (QB)': {'E911_Report_Col': '911 TVW - Blank', 'Output_Col': 'P_911_TVW__BLANK_QB'},
        'P:911 TVW - Complete (QB)': {'E911_Report_Col': '911 TVW - Complete', 'Output_Col': 'P_911_TVW__COMPLETE_QB'},
        'P:PSAP - FCC ID (QB)': {'E911_Report_Col': 'PSAP - FCC ID', 'Output_Col': 'P_PSAP__FCC_ID_QB'},
        'P:PSAP NAME (QB)': {'E911_Report_Col': 'PSAP NAME', 'Output_Col': 'P_PSAP_NAME_QB'},
        'P:PSAP E-911 CURRENT STATUS (QB)': {'E911_Report_Col': 'PSAP E-911 CURRENT STATUS', 'Output_Col': 'P_PSAP_E911_CURRENT_STATUS_QB'},
        'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)': {'E911_Report_Col': 'PSAP - FCC ID - PSAP ADMIN phone# (non-emergency)', 'Output_Col': 'P_PSAP_FCC_ID__PSAP_ADM_PHON'},
        'P:Tower Type (QB)': {'E911_Report_Col': 'Tower Type', 'Output_Col': 'P_TOWER_TYPE_QB'},
        'P:Building/Cabinet (QB)': {'E911_Report_Col': 'Building/Cabinet', 'Output_Col': 'P_BUILDING_CABINET_QB'},
        'P:FRN Cluster ID': {'E911_Report_Col': 'FRN Number', 'Output_Col': 'FRN Cluster ID'},
        'P:Samsung Opto Cluster (QB)': {'E911_Report_Col': 'Samsung Opto Cluster', 'Output_Col': 'P_SAMSUNG_OPTO_CLUSTER_QB'},
        'P:ESTIMATED BUILD YEAR (QB)': {'E911_Report_Col': 'ESTIMATED BUILD YEAR', 'Output_Col': 'P_ESTIMATED_BUILD_YEAR_QB'},
        'P:Tower Online (QB)': {'E911_Report_Col': 'Tower Online', 'Output_Col': 'P_TOWER_ONLINE_QB'},
        'P:Tower Online Date (QB)': {'E911_Report_Col': 'Tower Online Date', 'Output_Col': 'P_TOWER_ONLINE_DATE_QB'},
        'P:CAF-II site? (QB)': {'E911_Report_Col': 'CAF-II site?', 'Output_Col': 'P_CAFII_SITE_QB'},
        'P:Grant Site (QB)': {'E911_Report_Col': 'Grant Site', 'Output_Col': 'P_GRANT_SITE_QB'}
    }

    # Columns used for difference check (status/update fields)
    COMPARISON_COLS = [
        'P:TVW estimated ready/rough forecast date (QB)',
        'P:911 TVW - Blank (QB)',
        'P:911 TVW - Complete (QB)',
        'P:PSAP E-911 CURRENT STATUS (QB)',
        'P:Tower Online (QB)',
        'P:Tower Online Date (QB)'
    ]

    daily_cols = list(COLUMN_MAP.keys())
    output_cols_final_order = [v['Output_Col'] for v in COLUMN_MAP.values()]
    
    MERGE_KEY_COL = 'STANDARDIZED_SITE_ID' 
    DAILY_KEY = 'P:Viaero Root ID'
    REPORT_KEY = 'Site #'

    # 2. File Reading 
    try:
        # Assuming E911_Daily_ is CSV and E911_Report_ is Excel based on original code
        df_daily = pd.read_csv(daily_file_path)
        df_report = pd.read_excel(report_file_path, engine='openpyxl')
    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names by stripping whitespace
    df_daily.columns = df_daily.columns.str.strip()
    df_report.columns = df_report.columns.str.strip()

    # 3. Data Preparation and Standardization on the Merge Key
    if DAILY_KEY not in df_daily.columns or REPORT_KEY not in df_report.columns:
        print(f"Error: Required merge key columns ('{DAILY_KEY}' or '{REPORT_KEY}') are missing.")
        return
            
    # Standardize the merge key: string, strip, uppercase, zero-fill for robust matching
    df_daily[MERGE_KEY_COL] = df_daily[DAILY_KEY].astype(str).str.strip().str.upper().str.zfill(4)
    df_report[MERGE_KEY_COL] = df_report[REPORT_KEY].astype(str).str.strip().str.upper().str.zfill(4)

    # 4. Select and Rename Columns for Merging
    # Daily columns (Left side) get a '_daily' suffix
    daily_rename_map = {col: f"{col}_daily" for col in daily_cols}
    daily_final_cols = [col for col in daily_cols if col in df_daily.columns]
    df_daily_subset = df_daily[daily_final_cols + [MERGE_KEY_COL]].copy()
    df_daily_subset.rename(columns=daily_rename_map, inplace=True)
    
    # Report columns (Right side) used for comparison are selected by their original names
    report_relevant_cols = list(set(col_map['E911_Report_Col'] for col_map in COLUMN_MAP.values()))
    df_report_subset = df_report[[col for col in df_report.columns if col in report_relevant_cols] + [MERGE_KEY_COL]].copy()
    
    # 5. Merge the DataFrames (Left Merge)
    merged_df = pd.merge(
        df_daily_subset,
        df_report_subset,
        on=MERGE_KEY_COL, 
        how='left'
    )

    # 6. Discrepancy Finding Logic
    daily_comp_cols = [f"{col}_daily" for col in COMPARISON_COLS]
    report_comp_cols = [COLUMN_MAP[col]['E911_Report_Col'] for col in COMPARISON_COLS]
    
    daily_comp_cols_present = [col for col in daily_comp_cols if col in merged_df.columns]
    report_comp_cols_present = [col for col in report_comp_cols if col in merged_df.columns]

    # Concatenate key comparison fields to check for data mismatch
    merged_df['daily_concat'] = merged_df[daily_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)
    merged_df['report_concat'] = merged_df[report_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)

    # Check for missing record using the Report file's 'Project ID' (or any non-key report column)
    missing_report_key = COLUMN_MAP['P:Project ID']['E911_Report_Col'] 
    
    df_diff = merged_df[
        (merged_df['daily_concat'] != merged_df['report_concat']) | # Data mismatch
        (merged_df[missing_report_key].isna())                      # Record missing in Report file
    ].copy()
    
    print(f"Found {len(df_diff)} discrepancies.")

    # 7. Final Output Generation and Renaming
    if not df_diff.empty:
        # Select ALL columns from the Daily side of the merge (which are in df_diff)
        # Note: We use the *un-suffixed* DAILY_KEY (P:Viaero Root ID) for the merge key column
        output_source_cols = [f"{col}_daily" for col in daily_cols if col in daily_rename_map and col != DAILY_KEY]
        output_source_cols.insert(0, DAILY_KEY) # Insert the primary key
        
        df_output = df_diff[output_source_cols].copy()
        
        # Apply the final rename map to match the required 'Output' columns
        final_rename_map = {}
        for original_col, map_details in COLUMN_MAP.items():
            output_name = map_details['Output_Col']
            if original_col == DAILY_KEY:
                # P:Viaero Root ID is the un-suffixed key
                final_rename_map[DAILY_KEY] = output_name
            else:
                # All other columns are suffixed
                final_rename_map[f"{original_col}_daily"] = output_name
        
        df_output.rename(columns=final_rename_map, inplace=True)
        
        # Ensure the columns are in the correct final order
        df_output = df_output[output_cols_final_order]

        # Output File Generation
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_E911_{timestamp}.xlsx" 
        output_path = os.path.join(os.path.dirname(daily_file_path), output_file_name)
        
        df_output.to_excel(output_path, index=False)
        print(f"âœ… Final Discrepancy Report saved to: {output_path}")
        
==
        
    else:
        print("âœ… No differences found. No report generated.")

# Example Usage (You will need to set your actual file paths here)
if __name__ == '__main__':
    # Define placeholder paths for testing or local run
    # NOTE: Replace these with your actual file paths
    daily_file = r"path/to/your/E911_Daily_20251014.csv"
    report_file = r"path/to/your/E911_Report_Tracker_Data.xlsx"
    
    # process_e911_discrepancy_report(daily_file, report_file)
    print("\n--- Script finished. Please uncomment the example call and define actual paths to run the function. ---")

Update point 2

    # 2. File Reading (UPDATED to handle encoding errors)
    try:
        # Try reading with common encodings if the default (UTF-8) fails
        try:
            df_daily = pd.read_csv(daily_file_path)
        except UnicodeDecodeError:
            print("UTF-8 decode failed. Trying 'latin-1' encoding for E911_Daily_ file...")
            try:
                df_daily = pd.read_csv(daily_file_path, encoding='latin-1')
            except Exception:
                 print("Latin-1 decode failed. Trying 'cp1252' encoding...")
                 df_daily = pd.read_csv(daily_file_path, encoding='cp1252')
                 
        # E911_Report_ is assumed to be Excel, which handles encoding better
        df_report = pd.read_excel(report_file_path, engine='openpyxl')
        
    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return


error 
--- Starting comparison: E911_Report_2025-10-14_14-33-24.csv (Old) vs. E911_Daily_202510141222_53452.xlsx (New) ---
UTF-8 decode failed. Trying 'latin-1' encoding for E911_Daily_ file...
Error: Required merge key columns ('P:Viaero Root ID' or 'Site #') are missing.

====V3

import pandas as pd
import os
from datetime import datetime

def process_e911_discrepancy_report(file_path_1, file_path_2):
    """
    Compares two E911 files, intelligently determining which is the old (CSV) 
    and which is the new (Excel) source, identifies discrepancies, and outputs 
    a report with the required column mapping.
    
    Args:
        file_path_1 (str): Path to the first file (either Daily or Report).
        file_path_2 (str): Path to the second file (either Daily or Report).
    """
    print(f"\n--- Starting comparison between {os.path.basename(file_path_1)} and {os.path.basename(file_path_2)} ---")
    
    # 1. Define Column Mapping and Keys (Unchanged)
    COLUMN_MAP = {
        'P:Project ID': {'E911_Report_Col': 'Project ID', 'Output_Col': 'Project ID'},
        'P:FCC Site ID Number': {'E911_Report_Col': 'FCC Location ID', 'Output_Col': 'FCC Site ID'},
        'P:Viaero Root ID': {'E911_Report_Col': 'Site #', 'Output_Col': 'Viaero Root ID'},
        'P:Viaero Site Name': {'E911_Report_Col': 'Site Name', 'Output_Col': 'Viaero Site Name'},
        'P:TVW estimated ready/rough forecast date (QB)': {'E911_Report_Col': 'TVW estimated ready/rough forecast date:', 'Output_Col': 'P_TVW_EST_READY_FORE_QB'},
        'P:911 TVW - Blank (QB)': {'E911_Report_Col': '911 TVW - Blank', 'Output_Col': 'P_911_TVW__BLANK_QB'},
        'P:911 TVW - Complete (QB)': {'E911_Report_Col': '911 TVW - Complete', 'Output_Col': 'P_911_TVW__COMPLETE_QB'},
        'P:PSAP - FCC ID (QB)': {'E911_Report_Col': 'PSAP - FCC ID', 'Output_Col': 'P_PSAP__FCC_ID_QB'},
        'P:PSAP NAME (QB)': {'E911_Report_Col': 'PSAP NAME', 'Output_Col': 'P_PSAP_NAME_QB'},
        'P:PSAP E-911 CURRENT STATUS (QB)': {'E911_Report_Col': 'PSAP E-911 CURRENT STATUS', 'Output_Col': 'P_PSAP_E911_CURRENT_STATUS_QB'},
        'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)': {'E911_Report_Col': 'PSAP - FCC ID - PSAP ADMIN phone# (non-emergency)', 'Output_Col': 'P_PSAP_FCC_ID__PSAP_ADM_PHON'},
        'P:Tower Type (QB)': {'E911_Report_Col': 'Tower Type', 'Output_Col': 'P_TOWER_TYPE_QB'},
        'P:Building/Cabinet (QB)': {'E911_Report_Col': 'Building/Cabinet', 'Output_Col': 'P_BUILDING_CABINET_QB'},
        'P:FRN Cluster ID': {'E911_Report_Col': 'FRN Number', 'Output_Col': 'FRN Cluster ID'},
        'P:Samsung Opto Cluster (QB)': {'E911_Report_Col': 'Samsung Opto Cluster', 'Output_Col': 'P_SAMSUNG_OPTO_CLUSTER_QB'},
        'P:ESTIMATED BUILD YEAR (QB)': {'E911_Report_Col': 'ESTIMATED BUILD YEAR', 'Output_Col': 'P_ESTIMATED_BUILD_YEAR_QB'},
        'P:Tower Online (QB)': {'E911_Report_Col': 'Tower Online', 'Output_Col': 'P_TOWER_ONLINE_QB'},
        'P:Tower Online Date (QB)': {'E911_Report_Col': 'Tower Online Date', 'Output_Col': 'P_TOWER_ONLINE_DATE_QB'},
        'P:CAF-II site? (QB)': {'E911_Report_Col': 'CAF-II site?', 'Output_Col': 'P_CAFII_SITE_QB'},
        'P:Grant Site (QB)': {'E911_Report_Col': 'Grant Site', 'Output_Col': 'P_GRANT_SITE_QB'}
    }
    COMPARISON_COLS = [
        'P:TVW estimated ready/rough forecast date (QB)', 'P:911 TVW - Blank (QB)',
        'P:911 TVW - Complete (QB)', 'P:PSAP E-911 CURRENT STATUS (QB)',
        'P:Tower Online (QB)', 'P:Tower Online Date (QB)'
    ]
    daily_cols = list(COLUMN_MAP.keys())
    output_cols_final_order = [v['Output_Col'] for v in COLUMN_MAP.values()]
    MERGE_KEY_COL = 'STANDARDIZED_SITE_ID' 
    DAILY_KEY = 'P:Viaero Root ID'
    REPORT_KEY = 'Site #'

    # 2. File Reading & Role Assignment (FIXED)
    df_daily = None
    df_report = None
    
    # Helper to read CSV with encoding fallback
    def read_robust_csv(path):
        try:
            return pd.read_csv(path)
        except UnicodeDecodeError:
            try:
                print(f"UTF-8 decode failed for {os.path.basename(path)}. Trying 'latin-1'...")
                return pd.read_csv(path, encoding='latin-1')
            except Exception:
                print(f"Latin-1 decode failed for {os.path.basename(path)}. Trying 'cp1252'...")
                return pd.read_csv(path, encoding='cp1252')
    
    try:
        if file_path_1.lower().endswith('.csv') and file_path_2.lower().endswith(('.xlsx', '.xls')):
            # Standard order: file_path_1 (CSV) is OLD, file_path_2 (XLSX) is NEW
            df_daily = read_robust_csv(file_path_1)
            df_report = pd.read_excel(file_path_2, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_1)} is OLD (CSV), {os.path.basename(file_path_2)} is NEW (Excel) ---")
            
        elif file_path_2.lower().endswith('.csv') and file_path_1.lower().endswith(('.xlsx', '.xls')):
            # Reversed order: file_path_2 (CSV) is OLD, file_path_1 (XLSX) is NEW
            df_daily = read_robust_csv(file_path_2)
            df_report = pd.read_excel(file_path_1, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_2)} is OLD (CSV), {os.path.basename(file_path_1)} is NEW (Excel) ---")
            
        else:
            print("Error: Could not determine file roles. Expecting one CSV (OLD) and one Excel (NEW) file.")
            return

    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names by stripping whitespace
    df_daily.columns = df_daily.columns.str.strip()
    df_report.columns = df_report.columns.str.strip()

    # 3. Data Preparation and Standardization on the Merge Key
    # Now that roles are correct, we check for the right columns in the right DF.
    if DAILY_KEY not in df_daily.columns:
        print(f"Error: Required merge key column '{DAILY_KEY}' is missing from the OLD (CSV) file.")
        return
    if REPORT_KEY not in df_report.columns:
        print(f"Error: Required merge key column '{REPORT_KEY}' is missing from the NEW (Excel) file.")
        return
            
    # Standardize the merge key
    df_daily[MERGE_KEY_COL] = df_daily[DAILY_KEY].astype(str).str.strip().str.upper().str.zfill(4)
    df_report[MERGE_KEY_COL] = df_report[REPORT_KEY].astype(str).str.strip().str.upper().str.zfill(4)

    # 4. Select and Rename Columns for Merging
    # Daily columns (Left side) get a '_daily' suffix
    daily_rename_map = {col: f"{col}_daily" for col in daily_cols}
    daily_final_cols = [col for col in daily_cols if col in df_daily.columns]
    df_daily_subset = df_daily[daily_final_cols + [MERGE_KEY_COL]].copy()
    df_daily_subset.rename(columns=daily_rename_map, inplace=True)
    
    # Report columns (Right side) used for comparison are selected by their original names
    report_relevant_cols = list(set(col_map['E911_Report_Col'] for col_map in COLUMN_MAP.values()))
    df_report_subset = df_report[[col for col in df_report.columns if col in report_relevant_cols] + [MERGE_KEY_COL]].copy()
    
    # 5. Merge the DataFrames (Left Merge)
    merged_df = pd.merge(
        df_daily_subset,
        df_report_subset,
        on=MERGE_KEY_COL, 
        how='left'
    )

    # 6. Discrepancy Finding Logic
    daily_comp_cols = [f"{col}_daily" for col in COMPARISON_COLS]
    report_comp_cols = [COLUMN_MAP[col]['E911_Report_Col'] for col in COMPARISON_COLS]
    
    daily_comp_cols_present = [col for col in daily_comp_cols if col in merged_df.columns]
    report_comp_cols_present = [col for col in report_comp_cols if col in merged_df.columns]

    merged_df['daily_concat'] = merged_df[daily_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)
    merged_df['report_concat'] = merged_df[report_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)

    missing_report_key = COLUMN_MAP['P:Project ID']['E911_Report_Col'] 
    
    df_diff = merged_df[
        (merged_df['daily_concat'] != merged_df['report_concat']) |
        (merged_df[missing_report_key].isna())                      
    ].copy()
    
    print(f"Found {len(df_diff)} discrepancies.")

    # 7. Final Output Generation and Renaming
    if not df_diff.empty:
        output_source_cols = [f"{col}_daily" for col in daily_cols if col in daily_rename_map and col != DAILY_KEY]
        output_source_cols.insert(0, DAILY_KEY)
        
        df_output = df_diff[output_source_cols].copy()
        
        final_rename_map = {}
        for original_col, map_details in COLUMN_MAP.items():
            output_name = map_details['Output_Col']
            if original_col == DAILY_KEY:
                final_rename_map[DAILY_KEY] = output_name
            else:
                final_rename_map[f"{original_col}_daily"] = output_name
        
        df_output.rename(columns=final_rename_map, inplace=True)
        df_output = df_output[output_cols_final_order]

        # Output File Generation
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_E911_{timestamp}.xlsx" 
        output_path = os.path.join(os.path.dirname(os.path.abspath(file_path_1)), output_file_name)
        
        df_output.to_excel(output_path, index=False)
        print(f"âœ… Final Discrepancy Report saved to: {output_path}")
        
    else:
        print("âœ… No differences found. No report generated.")



--- Starting comparison between E911_Report_2025-10-14_14-33-24.csv and E911_Daily_202510141222_53452.xlsx ---
UTF-8 decode failed for E911_Report_2025-10-14_14-33-24.csv. Trying 'latin-1'...
--- Roles: E911_Report_2025-10-14_14-33-24.csv is OLD (CSV), E911_Daily_202510141222_53452.xlsx is NEW (Excel) ---
Error: Required merge key column 'P:Viaero Root ID' is missing from the OLD (CSV) file.


==V4

import pandas as pd
import os
from datetime import datetime

# NOTE: The 'upload_file_to_sftp' function is assumed to be defined elsewhere.

def process_e911_discrepancy_report(file_path_1, file_path_2):
    """
    Compares two E911 files, identifies discrepancies, and outputs a report 
    populated with the NEW/CORRECT data from the E911_Report_ file, applying 
    the required mapping, and standardizing the Site # key.
    
    Args:
        file_path_1 (str): Path to the first file (either Daily or Report).
        file_path_2 (str): Path to the second file (either Daily or Report).
    """
    print(f"\n--- Starting comparison between {os.path.basename(file_path_1)} and {os.path.basename(file_path_2)} ---")
    
    # 1. Define Column Mapping and Keys
    
    # Key columns in source files are both 'Site #'
    DAILY_KEY = 'Site #'
    REPORT_KEY = 'Site #'
    
    COLUMN_MAP = {
        'P:Project ID': {'E911_Report_Col': 'Project ID', 'Output_Col': 'Project ID'},
        'P:FCC Site ID Number': {'E911_Report_Col': 'FCC Location ID', 'Output_Col': 'FCC Site ID'},
        DAILY_KEY: {'E911_Report_Col': REPORT_KEY, 'Output_Col': 'Viaero Root ID'}, 
        'P:Viaero Site Name': {'E911_Report_Col': 'Site Name', 'Output_Col': 'Viaero Site Name'},
        'P:TVW estimated ready/rough forecast date (QB)': {'E911_Report_Col': 'TVW estimated ready/rough forecast date:', 'Output_Col': 'P_TVW_EST_READY_FORE_QB'},
        'P:911 TVW - Blank (QB)': {'E911_Report_Col': '911 TVW - Blank', 'Output_Col': 'P_911_TVW__BLANK_QB'},
        'P:911 TVW - Complete (QB)': {'E911_Report_Col': '911 TVW - Complete', 'Output_Col': 'P_911_TVW__COMPLETE_QB'},
        'P:PSAP - FCC ID (QB)': {'E911_Report_Col': 'PSAP - FCC ID', 'Output_Col': 'P_PSAP__FCC_ID_QB'},
        'P:PSAP NAME (QB)': {'E911_Report_Col': 'PSAP NAME', 'Output_Col': 'P_PSAP_NAME_QB'},
        'P:PSAP E-911 CURRENT STATUS (QB)': {'E911_Report_Col': 'PSAP E-911 CURRENT STATUS', 'Output_Col': 'P_PSAP_E911_CURRENT_STATUS_QB'},
        'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)': {'E911_Report_Col': 'PSAP - FCC ID - PSAP ADMIN phone# (non-emergency)', 'Output_Col': 'P_PSAP_FCC_ID__PSAP_ADM_PHON'},
        'P:Tower Type (QB)': {'E911_Report_Col': 'Tower Type', 'Output_Col': 'P_TOWER_TYPE_QB'},
        'P:Building/Cabinet (QB)': {'E911_Report_Col': 'Building/Cabinet', 'Output_Col': 'P_BUILDING_CABINET_QB'},
        'P:FRN Cluster ID': {'E911_Report_Col': 'FRN Number', 'Output_Col': 'FRN Cluster ID'},
        'P:Samsung Opto Cluster (QB)': {'E911_Report_Col': 'Samsung Opto Cluster', 'Output_Col': 'P_SAMSUNG_OPTO_CLUSTER_QB'},
        'P:ESTIMATED BUILD YEAR (QB)': {'E911_Report_Col': 'ESTIMATED BUILD YEAR', 'Output_Col': 'P_ESTIMATED_BUILD_YEAR_QB'},
        'P:Tower Online (QB)': {'E911_Report_Col': 'Tower Online', 'Output_Col': 'P_TOWER_ONLINE_QB'},
        'P:Tower Online Date (QB)': {'E911_Report_Col': 'Tower Online Date', 'Output_Col': 'P_TOWER_ONLINE_DATE_QB'},
        'P:CAF-II site? (QB)': {'E911_Report_Col': 'CAF-II site?', 'Output_Col': 'P_CAFII_SITE_QB'},
        'P:Grant Site (QB)': {'E911_Report_Col': 'Grant Site', 'Output_Col': 'P_GRANT_SITE_QB'}
    }

    COMPARISON_COLS = [
        'P:TVW estimated ready/rough forecast date (QB)', 'P:911 TVW - Blank (QB)',
        'P:911 TVW - Complete (QB)', 'P:PSAP E-911 CURRENT STATUS (QB)',
        'P:Tower Online (QB)', 'P:Tower Online Date (QB)'
    ]

    daily_cols = list(COLUMN_MAP.keys())
    output_cols_final_order = [v['Output_Col'] for v in COLUMN_MAP.values()]
    MERGE_KEY_COL = 'STANDARDIZED_SITE_ID' 

    # 2. File Reading & Role Assignment
    df_daily = None
    df_report = None
    
    def read_robust_csv(path):
        try:
            return pd.read_csv(path)
        except UnicodeDecodeError:
            try:
                print(f"UTF-8 decode failed for {os.path.basename(path)}. Trying 'latin-1'...")
                return pd.read_csv(path, encoding='latin-1')
            except Exception:
                print(f"Latin-1 decode failed for {os.path.basename(path)}. Trying 'cp1252'...")
                return pd.read_csv(path, encoding='cp1252')
    
    try:
        if file_path_1.lower().endswith('.csv') and file_path_2.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_1)
            df_report = pd.read_excel(file_path_2, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_1)} is OLD (CSV), {os.path.basename(file_path_2)} is NEW (Excel) ---")
            
        elif file_path_2.lower().endswith('.csv') and file_path_1.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_2)
            df_report = pd.read_excel(file_path_1, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_2)} is OLD (CSV), {os.path.basename(file_path_1)} is NEW (Excel) ---")
            
        else:
            print("Error: Could not determine file roles. Expecting one CSV (OLD) and one Excel (NEW) file.")
            return

    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names by stripping whitespace
    df_daily.columns = df_daily.columns.str.strip()
    df_report.columns = df_report.columns.str.strip()

    # 3. Data Preparation and Standardization on the Merge Key
    if DAILY_KEY not in df_daily.columns:
        print(f"Error: Required merge key column '{DAILY_KEY}' is missing from the OLD (CSV) file.")
        return
    if REPORT_KEY not in df_report.columns:
        print(f"Error: Required merge key column '{REPORT_KEY}' is missing from the NEW (Excel) file.")
        return
            
    # ZERO-PADDING FIX IS APPLIED HERE
    df_daily[MERGE_KEY_COL] = df_daily[DAILY_KEY].astype(str).str.strip().str.upper().str.zfill(4)
    df_report[MERGE_KEY_COL] = df_report[REPORT_KEY].astype(str).str.strip().str.upper().str.zfill(4)

    # 4. Select and Rename Columns for Merging
    daily_rename_map = {col: f"{col}_daily" for col in daily_cols}
    daily_final_cols = [col for col in daily_cols if col in df_daily.columns]
    df_daily_subset = df_daily[daily_final_cols + [MERGE_KEY_COL]].copy()
    df_daily_subset.rename(columns=daily_rename_map, inplace=True)
    
    report_relevant_cols = list(set(col_map['E911_Report_Col'] for col_map in COLUMN_MAP.values()))
    df_report_subset = df_report[[col for col in df_report.columns if col in report_relevant_cols] + [MERGE_KEY_COL]].copy()
    
    # 5. Merge the DataFrames (Left Merge)
    merged_df = pd.merge(
        df_daily_subset,
        df_report_subset,
        on=MERGE_KEY_COL, 
        how='left'
    )

    # 6. Discrepancy Finding Logic
    daily_comp_cols = [f"{col}_daily" for col in COMPARISON_COLS]
    report_comp_cols = [COLUMN_MAP[col]['E911_Report_Col'] for col in COMPARISON_COLS]
    
    daily_comp_cols_present = [col for col in daily_comp_cols if col in merged_df.columns]
    report_comp_cols_present = [col for col in report_comp_cols if col in merged_df.columns]

    merged_df['daily_concat'] = merged_df[daily_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)
    merged_df['report_concat'] = merged_df[report_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)

    missing_report_key = COLUMN_MAP['P:Project ID']['E911_Report_Col'] 
    
    df_diff = merged_df[
        (merged_df['daily_concat'] != merged_df['report_concat']) |
        (merged_df[missing_report_key].isna())                      
    ].copy()
    
    print(f"Found {len(df_diff)} discrepancies.")

    # 7. Final Output Generation and Renaming (Populated by NEW Data)
    if not df_diff.empty:
        
        output_source_map = {}
        for daily_col, map_details in COLUMN_MAP.items():
            output_col = map_details['Output_Col']
            report_col = map_details['E911_Report_Col']
            
            # Project ID is the only column coming from the OLD file, as requested.
            if daily_col == 'P:Project ID':
                output_source_map[f"{daily_col}_daily"] = output_col
            else:
                # All other columns, including Site # (Viaero Root ID), come from the NEW file.
                output_source_map[report_col] = output_col

        cols_to_select = list(output_source_map.keys())
        
        df_output = df_diff[cols_to_select].copy()
        
        # Apply the final rename map
        df_output.rename(columns=output_source_map, inplace=True)
        
        # Ensure the columns are in the correct final order
        df_output = df_output[output_cols_final_order]

        # Output File Generation
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_E911_{timestamp}.xlsx" 
        output_path = os.path.join(os.path.dirname(os.path.abspath(file_path_1)), output_file_name) 
        
        df_output.to_excel(output_path, index=False)
        print(f"âœ… Final New Data Report saved to: {output_path} (Populated with E911_Report data)")
        
    else:
        print("âœ… No differences found. No report generated.")


error 

--- Starting comparison between E911_Report_2025-10-14_14-33-24.csv and E911_Daily_202510141222_53452.xlsx ---
UTF-8 decode failed for E911_Report_2025-10-14_14-33-24.csv. Trying 'latin-1'...
--- Roles: E911_Report_2025-10-14_14-33-24.csv is OLD (CSV), E911_Daily_202510141222_53452.xlsx is NEW (Excel) ---
Error: Required merge key column 'Site #' is missing from the NEW (Excel) file.

=====V5

import pandas as pd
import os
from datetime import datetime

# NOTE: The 'upload_file_to_sftp' function is assumed to be defined elsewhere.

def process_e911_discrepancy_report(file_path_1, file_path_2):
    """
    Compares two E911 files, identifies discrepancies, and outputs a report 
    populated with the NEW/CORRECT data from the E911_Report_ file, applying 
    the required mapping, and standardizing the Site # key.
    
    Args:
        file_path_1 (str): Path to the first file (either Daily or Report).
        file_path_2 (str): Path to the second file (either Daily or Report).
    """
    print(f"\n--- Starting comparison between {os.path.basename(file_path_1)} and {os.path.basename(file_path_2)} ---")
    
    # 1. Define Column Mapping and Keys
    
    # Key columns in source files are expected to be 'Site #'
    DAILY_KEY = 'Site #'
    REPORT_KEY = 'Site #'
    
    COLUMN_MAP = {
        'P:Project ID': {'E911_Report_Col': 'Project ID', 'Output_Col': 'Project ID'},
        'P:FCC Site ID Number': {'E911_Report_Col': 'FCC Location ID', 'Output_Col': 'FCC Site ID'},
        DAILY_KEY: {'E911_Report_Col': REPORT_KEY, 'Output_Col': 'Viaero Root ID'}, 
        'P:Viaero Site Name': {'E911_Report_Col': 'Site Name', 'Output_Col': 'Viaero Site Name'},
        'P:TVW estimated ready/rough forecast date (QB)': {'E911_Report_Col': 'TVW estimated ready/rough forecast date:', 'Output_Col': 'P_TVW_EST_READY_FORE_QB'},
        'P:911 TVW - Blank (QB)': {'E911_Report_Col': '911 TVW - Blank', 'Output_Col': 'P_911_TVW__BLANK_QB'},
        'P:911 TVW - Complete (QB)': {'E911_Report_Col': '911 TVW - Complete', 'Output_Col': 'P_911_TVW__COMPLETE_QB'},
        'P:PSAP - FCC ID (QB)': {'E911_Report_Col': 'PSAP - FCC ID', 'Output_Col': 'P_PSAP__FCC_ID_QB'},
        'P:PSAP NAME (QB)': {'E911_Report_Col': 'PSAP NAME', 'Output_Col': 'P_PSAP_NAME_QB'},
        'P:PSAP E-911 CURRENT STATUS (QB)': {'E911_Report_Col': 'PSAP E-911 CURRENT STATUS', 'Output_Col': 'P_PSAP_E911_CURRENT_STATUS_QB'},
        'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)': {'E911_Report_Col': 'PSAP - FCC ID - PSAP ADMIN phone# (non-emergency)', 'Output_Col': 'P_PSAP_FCC_ID__PSAP_ADM_PHON'},
        'P:Tower Type (QB)': {'E911_Report_Col': 'Tower Type', 'Output_Col': 'P_TOWER_TYPE_QB'},
        'P:Building/Cabinet (QB)': {'E911_Report_Col': 'Building/Cabinet', 'Output_Col': 'P_BUILDING_CABINET_QB'},
        'P:FRN Cluster ID': {'E911_Report_Col': 'FRN Number', 'Output_Col': 'FRN Cluster ID'},
        'P:Samsung Opto Cluster (QB)': {'E911_Report_Col': 'Samsung Opto Cluster', 'Output_Col': 'P_SAMSUNG_OPTO_CLUSTER_QB'},
        'P:ESTIMATED BUILD YEAR (QB)': {'E911_Report_Col': 'ESTIMATED BUILD YEAR', 'Output_Col': 'P_ESTIMATED_BUILD_YEAR_QB'},
        'P:Tower Online (QB)': {'E911_Report_Col': 'Tower Online', 'Output_Col': 'P_TOWER_ONLINE_QB'},
        'P:Tower Online Date (QB)': {'E911_Report_Col': 'Tower Online Date', 'Output_Col': 'P_TOWER_ONLINE_DATE_QB'},
        'P:CAF-II site? (QB)': {'E911_Report_Col': 'CAF-II site?', 'Output_Col': 'P_CAFII_SITE_QB'},
        'P:Grant Site (QB)': {'E911_Report_Col': 'Grant Site', 'Output_Col': 'P_GRANT_SITE_QB'}
    }

    COMPARISON_COLS = [
        'P:TVW estimated ready/rough forecast date (QB)', 'P:911 TVW - Blank (QB)',
        'P:911 TVW - Complete (QB)', 'P:PSAP E-911 CURRENT STATUS (QB)',
        'P:Tower Online (QB)', 'P:Tower Online Date (QB)'
    ]

    daily_cols = list(COLUMN_MAP.keys())
    output_cols_final_order = [v['Output_Col'] for v in COLUMN_MAP.values()]
    MERGE_KEY_COL = 'STANDARDIZED_SITE_ID' 

    # 2. File Reading & Role Assignment
    df_daily = None
    df_report = None
    
    def read_robust_csv(path):
        try:
            return pd.read_csv(path)
        except UnicodeDecodeError:
            try:
                print(f"UTF-8 decode failed for {os.path.basename(path)}. Trying 'latin-1'...")
                return pd.read_csv(path, encoding='latin-1')
            except Exception:
                print(f"Latin-1 decode failed for {os.path.basename(path)}. Trying 'cp1252'...")
                return pd.read_csv(path, encoding='cp1252')
    
    try:
        if file_path_1.lower().endswith('.csv') and file_path_2.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_1)
            df_report = pd.read_excel(file_path_2, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_1)} is OLD (CSV), {os.path.basename(file_path_2)} is NEW (Excel) ---")
            
        elif file_path_2.lower().endswith('.csv') and file_path_1.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_2)
            df_report = pd.read_excel(file_path_1, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_2)} is OLD (CSV), {os.path.basename(file_path_1)} is NEW (Excel) ---")
            
        else:
            print("Error: Could not determine file roles. Expecting one CSV (OLD) and one Excel (NEW) file.")
            return

    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names by stripping whitespace
    df_daily.columns = df_daily.columns.str.strip()
    df_report.columns = df_report.columns.str.strip()

    # 3. Data Preparation and Standardization on the Merge Key (IMPROVED CHECK)
    
    # Check for the key in the OLD file (CSV)
    if DAILY_KEY not in df_daily.columns:
        print(f"Error: Required merge key column '{DAILY_KEY}' is missing from the OLD (CSV) file.")
        print(f"Available columns in OLD file: {list(df_daily.columns)}")
        return
    
    # Check for the key in the NEW file (Excel)
    if REPORT_KEY not in df_report.columns:
        # **NEW Robust Check:** Find the closest match in the Report file
        report_cols_upper = [col.upper() for col in df_report.columns]
        
        # Look for columns that contain 'SITE' AND '#'
        match = [col for col in df_report.columns if 'SITE' in col.upper() and '#' in col]
        
        if match:
            # If a match is found, assume it is the key and remap REPORT_KEY
            new_report_key = match[0]
            
            # CRITICALLY: Update the COLUMN_MAP entry for the key
            for key, val in COLUMN_MAP.items():
                if val['E911_Report_Col'] == REPORT_KEY:
                    COLUMN_MAP[key]['E911_Report_Col'] = new_report_key
                    REPORT_KEY = new_report_key
                    print(f"ðŸ”‘ **SUCCESS**: Found merge key in NEW file: '{REPORT_KEY}'. Continuing with analysis.")
                    break
        else:
            # If no robust match is found, raise the error
            print(f"Error: Required merge key column '{REPORT_KEY}' is missing from the NEW (Excel) file.")
            print(f"Available columns in NEW file: {list(df_report.columns)}")
            print("Please confirm the exact name of the Site ID/Root ID column in your Excel file.")
            return
            
    # ZERO-PADDING FIX IS APPLIED HERE
    df_daily[MERGE_KEY_COL] = df_daily[DAILY_KEY].astype(str).str.strip().str.upper().str.zfill(4)
    df_report[MERGE_KEY_COL] = df_report[REPORT_KEY].astype(str).str.strip().str.upper().str.zfill(4)

    # 4. Select and Rename Columns for Merging
    daily_rename_map = {col: f"{col}_daily" for col in daily_cols}
    daily_final_cols = [col for col in daily_cols if col in df_daily.columns]
    df_daily_subset = df_daily[daily_final_cols + [MERGE_KEY_COL]].copy()
    df_daily_subset.rename(columns=daily_rename_map, inplace=True)
    
    # The columns to select from df_report are now based on the potentially updated REPORT_KEY
    report_relevant_cols = list(set(col_map['E911_Report_Col'] for col_map in COLUMN_MAP.values()))
    df_report_subset = df_report[[col for col in df_report.columns if col in report_relevant_cols] + [MERGE_KEY_COL]].copy()
    
    # 5. Merge the DataFrames (Left Merge)
    merged_df = pd.merge(
        df_daily_subset,
        df_report_subset,
        on=MERGE_KEY_COL, 
        how='left'
    )

    # 6. Discrepancy Finding Logic
    daily_comp_cols = [f"{col}_daily" for col in COMPARISON_COLS]
    # The report columns for comparison must be re-evaluated based on the potentially updated REPORT_KEY
    report_comp_cols = [COLUMN_MAP[col]['E911_Report_Col'] for col in COMPARISON_COLS]
    
    daily_comp_cols_present = [col for col in daily_comp_cols if col in merged_df.columns]
    report_comp_cols_present = [col for col in report_comp_cols if col in merged_df.columns]

    merged_df['daily_concat'] = merged_df[daily_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)
    merged_df['report_concat'] = merged_df[report_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)

    # The missing key check uses a non-key column from the report side
    missing_report_key = COLUMN_MAP['P:Project ID']['E911_Report_Col'] 
    
    df_diff = merged_df[
        (merged_df['daily_concat'] != merged_df['report_concat']) |
        (merged_df[missing_report_key].isna())                      
    ].copy()
    
    print(f"Found {len(df_diff)} discrepancies.")

    # 7. Final Output Generation and Renaming (Populated by NEW Data)
    if not df_diff.empty:
        
        output_source_map = {}
        for daily_col, map_details in COLUMN_MAP.items():
            output_col = map_details['Output_Col']
            report_col = map_details['E911_Report_Col']
            
            # Project ID is the only column coming from the OLD file, as requested.
            if daily_col == 'P:Project ID':
                output_source_map[f"{daily_col}_daily"] = output_col
            else:
                # All other columns, including Site # (Viaero Root ID), come from the NEW file.
                output_source_map[report_col] = output_col

        cols_to_select = list(output_source_map.keys())
        
        df_output = df_diff[cols_to_select].copy()
        
        # Apply the final rename map
        df_output.rename(columns=output_source_map, inplace=True)
        
        # Ensure the columns are in the correct final order
        df_output = df_output[output_cols_final_order]

        # Output File Generation
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_E911_{timestamp}.xlsx" 
        output_path = os.path.join(os.path.dirname(os.path.abspath(file_path_1)), output_file_name) 
        
        df_output.to_excel(output_path, index=False)
        print(f"âœ… Final New Data Report saved to: {output_path} (Populated with E911_Report data)")
        
    else:
        print("âœ… No differences found. No report generated.")


--- Starting comparison between E911_Report_2025-10-14_14-33-24.csv and E911_Daily_202510141222_53452.xlsx ---
UTF-8 decode failed for E911_Report_2025-10-14_14-33-24.csv. Trying 'latin-1'...
--- Roles: E911_Report_2025-10-14_14-33-24.csv is OLD (CSV), E911_Daily_202510141222_53452.xlsx is NEW (Excel) ---
Error: Required merge key column 'Site #' is missing from the NEW (Excel) file.
Available columns in NEW file: ['P:Project ID', 'P:FCC Site ID Number', 'P:Viaero Root ID', 'P:Viaero Site Name', 'P:TVW estimated ready/rough forecast date (QB)', 'P:911 TVW - Blank (QB)', 'P:911 TVW - Complete (QB)', 'P:PSAP - FCC ID (QB)', 'P:PSAP NAME (QB)', 'P:PSAP E-911 CURRENT STATUS (QB)', 'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)', 'P:Tower Type (QB)', 'P:Building/Cabinet (QB)', 'P:FRN Cluster ID', 'P:Samsung Opto Cluster (QB)', 'P:ESTIMATED BUILD YEAR (QB)', 'P:Tower Online (QB)', 'P:Tower Online Date (QB)', 'P:CAF-II site? (QB)', 'P:Grant Site (QB)']
Please confirm the exact name of the Site ID/Root ID column in your Excel file.



----v6

import pandas as pd
import os
from datetime import datetime

# NOTE: The 'upload_file_to_sftp' function is assumed to be defined elsewhere.

def process_e911_discrepancy_report(file_path_1, file_path_2):
    """
    Compares two E911 files, identifies discrepancies, and outputs a report 
    populated with the NEW/CORRECT data from the E911_Report_ file, applying 
    the required mapping. The column mapping is adjusted because the NEW (Excel) 
    file contains the OLD (Daily) column headers, and vice versa.
    
    Args:
        file_path_1 (str): Path to the first file (either Daily or Report).
        file_path_2 (str): Path to the second file (either Daily or Report).
    """
    print(f"\n--- Starting comparison between {os.path.basename(file_path_1)} and {os.path.basename(file_path_2)} ---")
    
    # 1. Define Column Mapping and Keys
    
    # BASED ON YOUR OUTPUT, THE FILE ROLES MUST BE SWAPPED IN TERMS OF HEADERS:
    # OLD data is coming from the CSV, but its headers are the Report headers (Site #).
    # NEW data is coming from the Excel, but its headers are the Daily/Quickbase headers (P:Viaero Root ID).
    
    # Daily (OLD/CSV) file uses Report's keys (Site #)
    DAILY_KEY_HEADER = 'Site #'
    # Report (NEW/Excel) file uses Daily's keys (P:Viaero Root ID)
    REPORT_KEY_HEADER = 'P:Viaero Root ID'
    
    # NOTE: The keys in the COLUMN_MAP must reflect the headers in the OLD (CSV) file.
    # The E911_Report_Col must reflect the headers in the NEW (Excel) file.
    
    COLUMN_MAP = {
        # Old (CSV) Header (Report Header) : {New (Excel) Header (Daily Header), Final Output Name}
        'Project ID': {'E911_Report_Col': 'P:Project ID', 'Output_Col': 'Project ID'},
        'FCC Location ID': {'E911_Report_Col': 'P:FCC Site ID Number', 'Output_Col': 'FCC Site ID'},
        DAILY_KEY_HEADER: {'E911_Report_Col': REPORT_KEY_HEADER, 'Output_Col': 'Viaero Root ID'}, 
        'Site Name': {'E911_Report_Col': 'P:Viaero Site Name', 'Output_Col': 'Viaero Site Name'},
        'TVW estimated ready/rough forecast date:': {'E911_Report_Col': 'P:TVW estimated ready/rough forecast date (QB)', 'Output_Col': 'P_TVW_EST_READY_FORE_QB'},
        '911 TVW - Blank': {'E911_Report_Col': 'P:911 TVW - Blank (QB)', 'Output_Col': 'P_911_TVW__BLANK_QB'},
        '911 TVW - Complete': {'E911_Report_Col': 'P:911 TVW - Complete (QB)', 'Output_Col': 'P_911_TVW__COMPLETE_QB'},
        'PSAP - FCC ID': {'E911_Report_Col': 'P:PSAP - FCC ID (QB)', 'Output_Col': 'P_PSAP__FCC_ID_QB'},
        'PSAP NAME': {'E911_Report_Col': 'P:PSAP NAME (QB)', 'Output_Col': 'P_PSAP_NAME_QB'},
        'PSAP E-911 CURRENT STATUS': {'E911_Report_Col': 'P:PSAP E-911 CURRENT STATUS (QB)', 'Output_Col': 'P_PSAP_E911_CURRENT_STATUS_QB'},
        'PSAP - FCC ID - PSAP ADMIN phone# (non-emergency)': {'E911_Report_Col': 'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)', 'Output_Col': 'P_PSAP_FCC_ID__PSAP_ADM_PHON'},
        'Tower Type': {'E911_Report_Col': 'P:Tower Type (QB)', 'Output_Col': 'P_TOWER_TYPE_QB'},
        'Building/Cabinet': {'E911_Report_Col': 'P:Building/Cabinet (QB)', 'Output_Col': 'P_BUILDING_CABINET_QB'},
        'FRN Number': {'E911_Report_Col': 'P:FRN Cluster ID', 'Output_Col': 'FRN Cluster ID'},
        'Samsung Opto Cluster': {'E911_Report_Col': 'P:Samsung Opto Cluster (QB)', 'Output_Col': 'P_SAMSUNG_OPTO_CLUSTER_QB'},
        'ESTIMATED BUILD YEAR': {'E911_Report_Col': 'P:ESTIMATED BUILD YEAR (QB)', 'Output_Col': 'P_ESTIMATED_BUILD_YEAR_QB'},
        'Tower Online': {'E911_Report_Col': 'P:Tower Online (QB)', 'Output_Col': 'P_TOWER_ONLINE_QB'},
        'Tower Online Date': {'E911_Report_Col': 'P:Tower Online Date (QB)', 'Output_Col': 'P_TOWER_ONLINE_DATE_QB'},
        'CAF-II site?': {'E911_Report_Col': 'P:CAF-II site? (QB)', 'Output_Col': 'P_CAFII_SITE_QB'},
        'Grant Site': {'E911_Report_Col': 'P:Grant Site (QB)', 'Output_Col': 'P_GRANT_SITE_QB'}
    }

    # Columns used for difference check (OLD data headers)
    COMPARISON_COLS = [
        'TVW estimated ready/rough forecast date:', '911 TVW - Blank',
        '911 TVW - Complete', 'PSAP E-911 CURRENT STATUS',
        'Tower Online', 'Tower Online Date'
    ]

    daily_cols = list(COLUMN_MAP.keys())
    output_cols_final_order = [v['Output_Col'] for v in COLUMN_MAP.values()]
    MERGE_KEY_COL = 'STANDARDIZED_SITE_ID' 
    
    # 2. File Reading & Role Assignment (INTELLIGENT, UNCHANGED)
    df_daily = None
    df_report = None
    
    def read_robust_csv(path):
        try:
            return pd.read_csv(path)
        except UnicodeDecodeError:
            try:
                print(f"UTF-8 decode failed for {os.path.basename(path)}. Trying 'latin-1'...")
                return pd.read_csv(path, encoding='latin-1')
            except Exception:
                print(f"Latin-1 decode failed for {os.path.basename(path)}. Trying 'cp1252'...")
                return pd.read_csv(path, encoding='cp1252')
    
    try:
        if file_path_1.lower().endswith('.csv') and file_path_2.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_1)
            df_report = pd.read_excel(file_path_2, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_1)} is OLD (CSV), {os.path.basename(file_path_2)} is NEW (Excel) ---")
            
        elif file_path_2.lower().endswith('.csv') and file_path_1.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_2)
            df_report = pd.read_excel(file_path_1, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_2)} is OLD (CSV), {os.path.basename(file_path_1)} is NEW (Excel) ---")
            
        else:
            print("Error: Could not determine file roles. Expecting one CSV (OLD) and one Excel (NEW) file.")
            return

    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names by stripping whitespace
    df_daily.columns = df_daily.columns.str.strip()
    df_report.columns = df_report.columns.str.strip()

    # 3. Data Preparation and Standardization on the Merge Key (Final Check)
    # Check for the key in the OLD file (CSV)
    if DAILY_KEY_HEADER not in df_daily.columns:
        print(f"Error: Required merge key column '{DAILY_KEY_HEADER}' is missing from the OLD (CSV) file.")
        print(f"Available columns in OLD file: {list(df_daily.columns)}")
        return
    
    # Check for the key in the NEW file (Excel)
    if REPORT_KEY_HEADER not in df_report.columns:
        # We rely on the exact name now, as the list of columns was provided
        print(f"Error: Required merge key column '{REPORT_KEY_HEADER}' is missing from the NEW (Excel) file.")
        print(f"Available columns in NEW file: {list(df_report.columns)}")
        print("Please ensure the column names match the map exactly.")
        return
            
    # ZERO-PADDING FIX IS APPLIED HERE
    df_daily[MERGE_KEY_COL] = df_daily[DAILY_KEY_HEADER].astype(str).str.strip().str.upper().str.zfill(4)
    df_report[MERGE_KEY_COL] = df_report[REPORT_KEY_HEADER].astype(str).str.strip().str.upper().str.zfill(4)

    # 4. Select and Rename Columns for Merging
    # Daily columns (Left side) get a '_daily' suffix for comparison
    daily_rename_map = {col: f"{col}_daily" for col in daily_cols}
    daily_final_cols = [col for col in daily_cols if col in df_daily.columns]
    df_daily_subset = df_daily[daily_final_cols + [MERGE_KEY_COL]].copy()
    df_daily_subset.rename(columns=daily_rename_map, inplace=True)
    
    # Report columns (Right side) used for comparison AND output selection
    report_relevant_cols = list(set(col_map['E911_Report_Col'] for col_map in COLUMN_MAP.values()))
    df_report_subset = df_report[[col for col in df_report.columns if col in report_relevant_cols] + [MERGE_KEY_COL]].copy()
    
    # 5. Merge the DataFrames (Left Merge)
    merged_df = pd.merge(
        df_daily_subset,
        df_report_subset,
        on=MERGE_KEY_COL, 
        how='left'
    )

    # 6. Discrepancy Finding Logic
    daily_comp_cols = [f"{col}_daily" for col in COMPARISON_COLS]
    report_comp_cols = [COLUMN_MAP[col]['E911_Report_Col'] for col in COMPARISON_COLS]
    
    daily_comp_cols_present = [col for col in daily_comp_cols if col in merged_df.columns]
    report_comp_cols_present = [col for col in report_comp_cols if col in merged_df.columns]

    merged_df['daily_concat'] = merged_df[daily_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)
    merged_df['report_concat'] = merged_df[report_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)

    # The missing key check uses a non-key column from the report side
    missing_report_key = COLUMN_MAP['Project ID']['E911_Report_Col'] 
    
    df_diff = merged_df[
        (merged_df['daily_concat'] != merged_df['report_concat']) |
        (merged_df[missing_report_key].isna())                      
    ].copy()
    
    print(f"Found {len(df_diff)} discrepancies.")

    # 7. Final Output Generation and Renaming (Populated by NEW Data)
    if not df_diff.empty:
        
        output_source_map = {}
        for daily_col, map_details in COLUMN_MAP.items():
            output_col = map_details['Output_Col']
            report_col = map_details['E911_Report_Col']
            
            # Project ID is the only column coming from the OLD file, as requested (NOTE: Renamed to match the new map key)
            if daily_col == 'Project ID':
                output_source_map[f"{daily_col}_daily"] = output_col
            else:
                # All other columns, including the key, come from the NEW file.
                output_source_map[report_col] = output_col

        cols_to_select = list(output_source_map.keys())
        
        df_output = df_diff[cols_to_select].copy()
        
        # Apply the final rename map
        df_output.rename(columns=output_source_map, inplace=True)
        
        # Ensure the columns are in the correct final order
        df_output = df_output[output_cols_final_order]

        # Output File Generation
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_E911_{timestamp}.xlsx" 
        output_path = os.path.join(os.path.dirname(os.path.abspath(file_path_1)), output_file_name) 
        
        df_output.to_excel(output_path, index=False)
        print(f"âœ… Final New Data Report saved to: {output_path} (Populated with E911_Daily/Quickbase data)")
        
    else:
        print("âœ… No differences found. No report generated.")
==Update piont 7

# ... (rest of the function is the same)

    # 7. Final Output Generation and Renaming (Populated by NEW Data)
    if not df_diff.empty:
        
        output_source_map = {}
        for daily_col, map_details in COLUMN_MAP.items():
            output_col = map_details['Output_Col']
            report_col = map_details['E911_Report_Col']
            
            # --- CRITICAL FIX HERE ---
            # Project ID (the only column from OLD) comes from the suffixed column.
            if daily_col == 'Project ID':
                # We need to explicitly use the key of the report column (which holds the P:Project ID header)
                # to get the correct suffixed column name from the merge.
                
                # Check if the Report (NEW) column header has the P: prefix, and use that as the base for the OLD column.
                if report_col.startswith('P:'):
                    old_suffixed_col = f"{report_col}_daily"
                else:
                    old_suffixed_col = f"{daily_col}_daily"
                    
                # The Project ID column in the merged DF must be named 'P:Project ID_daily'
                output_source_map[old_suffixed_col] = output_col
                
            else:
                # All other columns, including the key, come from the NEW file.
                output_source_map[report_col] = output_col

        cols_to_select = list(output_source_map.keys())
        
        # Check if the missing column is in the df_diff index before proceeding
        missing_cols = [col for col in cols_to_select if col not in df_diff.columns]
        if missing_cols:
             print(f"FATAL ERROR: The following expected column(s) are missing from the merged data: {missing_cols}")
             print("Please double-check the column names in both source files against the COLUMN_MAP.")
             return
        
        df_output = df_diff[cols_to_select].copy()
        
# ... (rest of the function is the same)

===v7

import pandas as pd
import os
from datetime import datetime

def process_e911_discrepancy_report(file_path_1, file_path_2):
    """
    Compares two E911 files, identifies discrepancies, and outputs a report 
    populated with the NEW/CORRECT data for updates. Project ID is exclusively 
    sourced from the OLD (CSV) file.
    
    Args:
        file_path_1 (str): Path to the first file (either Daily or Report).
        file_path_2 (str): Path to the second file (either Daily or Report).
    """
    print(f"\n--- Starting comparison between {os.path.basename(file_path_1)} and {os.path.basename(file_path_2)} ---")
    
    # 1. Define Column Mapping and Keys
    
    # OLD (CSV) file uses Report's keys (Site #)
    DAILY_KEY_HEADER = 'Site #'
    # NEW (Excel) file uses Daily's keys (P:Viaero Root ID)
    REPORT_KEY_HEADER = 'P:Viaero Root ID'
    
    COLUMN_MAP = {
        # Old (CSV) Header (Report Header) : {New (Excel) Header (Daily Header), Final Output Name}
        'Project ID': {'E911_Report_Col': 'P:Project ID', 'Output_Col': 'Project ID'}, 
        'FCC Location ID': {'E911_Report_Col': 'P:FCC Site ID Number', 'Output_Col': 'FCC Site ID'},
        DAILY_KEY_HEADER: {'E911_Report_Col': REPORT_KEY_HEADER, 'Output_Col': 'Viaero Root ID'}, 
        'Site Name': {'E911_Report_Col': 'P:Viaero Site Name', 'Output_Col': 'Viaero Site Name'},
        'TVW estimated ready/rough forecast date:': {'E911_Report_Col': 'P:TVW estimated ready/rough forecast date (QB)', 'Output_Col': 'P_TVW_EST_READY_FORE_QB'},
        '911 TVW - Blank': {'E911_Report_Col': 'P:911 TVW - Blank (QB)', 'Output_Col': 'P_911_TVW__BLANK_QB'},
        '911 TVW - Complete': {'E911_Report_Col': 'P:911 TVW - Complete (QB)', 'Output_Col': 'P_911_TVW__COMPLETE_QB'},
        'PSAP - FCC ID': {'E911_Report_Col': 'P:PSAP - FCC ID (QB)', 'Output_Col': 'P_PSAP__FCC_ID_QB'},
        'PSAP NAME': {'E911_Report_Col': 'P:PSAP NAME (QB)', 'Output_Col': 'P_PSAP_NAME_QB'},
        'PSAP E-911 CURRENT STATUS': {'E911_Report_Col': 'P:PSAP E-911 CURRENT STATUS (QB)', 'Output_Col': 'P_PSAP_E911_CURRENT_STATUS_QB'},
        'PSAP - FCC ID - PSAP ADMIN phone# (non-emergency)': {'E911_Report_Col': 'P:PSAP - FCC ID - PSAP ADMIN phone# (non-emergency) (QB)', 'Output_Col': 'P_PSAP_FCC_ID__PSAP_ADM_PHON'},
        'Tower Type': {'E911_Report_Col': 'P:Tower Type (QB)', 'Output_Col': 'P_TOWER_TYPE_QB'},
        'Building/Cabinet': {'E911_Report_Col': 'P:Building/Cabinet (QB)', 'Output_Col': 'P_BUILDING_CABINET_QB'},
        'FRN Number': {'E911_Report_Col': 'P:FRN Cluster ID', 'Output_Col': 'FRN Cluster ID'},
        'Samsung Opto Cluster': {'E911_Report_Col': 'P:Samsung Opto Cluster (QB)', 'Output_Col': 'P_SAMSUNG_OPTO_CLUSTER_QB'},
        'ESTIMATED BUILD YEAR': {'E911_Report_Col': 'P:ESTIMATED BUILD YEAR (QB)', 'Output_Col': 'P_ESTIMATED_BUILD_YEAR_QB'},
        'Tower Online': {'E911_Report_Col': 'P:Tower Online (QB)', 'Output_Col': 'P_TOWER_ONLINE_QB'},
        'Tower Online Date': {'E911_Report_Col': 'P:Tower Online Date (QB)', 'Output_Col': 'P_TOWER_ONLINE_DATE_QB'},
        'CAF-II site?': {'E911_Report_Col': 'P:CAF-II site? (QB)', 'Output_Col': 'P_CAFII_SITE_QB'},
        'Grant Site': {'E911_Report_Col': 'P:Grant Site (QB)', 'Output_Col': 'P_GRANT_SITE_QB'}
    }

    # Columns used for discrepancy check (OLD data headers)
    COMPARISON_COLS = [
        'TVW estimated ready/rough forecast date:', '911 TVW - Blank',
        '911 TVW - Complete', 'PSAP E-911 CURRENT STATUS',
        'Tower Online', 'Tower Online Date'
    ]

    daily_cols = list(COLUMN_MAP.keys())
    output_cols_final_order = [v['Output_Col'] for v in COLUMN_MAP.values()]
    MERGE_KEY_COL = 'STANDARDIZED_SITE_ID' 
    
    # 2. File Reading & Role Assignment (UNCHANGED)
    df_daily = None
    df_report = None
    
    def read_robust_csv(path):
        try:
            return pd.read_csv(path)
        except UnicodeDecodeError:
            try:
                print(f"UTF-8 decode failed for {os.path.basename(path)}. Trying 'latin-1'...")
                return pd.read_csv(path, encoding='latin-1')
            except Exception:
                print(f"Latin-1 decode failed for {os.path.basename(path)}. Trying 'cp1252'...")
                return pd.read_csv(path, encoding='cp1252')
    
    try:
        if file_path_1.lower().endswith('.csv') and file_path_2.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_1)
            df_report = pd.read_excel(file_path_2, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_1)} is OLD (CSV), {os.path.basename(file_path_2)} is NEW (Excel) ---")
            
        elif file_path_2.lower().endswith('.csv') and file_path_1.lower().endswith(('.xlsx', '.xls')):
            df_daily = read_robust_csv(file_path_2)
            df_report = pd.read_excel(file_path_1, engine='openpyxl')
            print(f"--- Roles: {os.path.basename(file_path_2)} is OLD (CSV), {os.path.basename(file_path_1)} is NEW (Excel) ---")
            
        else:
            print("Error: Could not determine file roles. Expecting one CSV (OLD) and one Excel (NEW) file.")
            return

    except FileNotFoundError as e:
        print(f"Error: One of the files was not found. Details: {e}")
        return
    except Exception as e:
        print(f"Error reading files: {e}")
        return

    # Clean column names by stripping whitespace
    df_daily.columns = df_daily.columns.str.strip()
    df_report.columns = df_report.columns.str.strip()

    # 3. Data Preparation and Standardization on the Merge Key (UNCHANGED)
    if DAILY_KEY_HEADER not in df_daily.columns:
        print(f"Error: Required merge key column '{DAILY_KEY_HEADER}' is missing from the OLD (CSV) file.")
        print(f"Available columns in OLD file: {list(df_daily.columns)}")
        return
    
    if REPORT_KEY_HEADER not in df_report.columns:
        print(f"Error: Required merge key column '{REPORT_KEY_HEADER}' is missing from the NEW (Excel) file.")
        print(f"Available columns in NEW file: {list(df_report.columns)}")
        print("Please ensure the column names match the map exactly.")
        return
            
    df_daily[MERGE_KEY_COL] = df_daily[DAILY_KEY_HEADER].astype(str).str.strip().str.upper().str.zfill(4)
    df_report[MERGE_KEY_COL] = df_report[REPORT_KEY_HEADER].astype(str).str.strip().str.upper().str.zfill(4)

    # 4. Select and Rename Columns for Merging (ADJUSTED for Project ID)
    daily_rename_map = {col: f"{col}_daily" for col in daily_cols}
    daily_final_cols = [col for col in daily_cols if col in df_daily.columns]
    df_daily_subset = df_daily[daily_final_cols + [MERGE_KEY_COL]].copy()
    df_daily_subset.rename(columns=daily_rename_map, inplace=True)
    
    # Report columns (Right side) used for comparison AND output selection
    # REMOVE Project ID from the columns we select from the NEW file
    report_relevant_cols = list(set(col_map['E911_Report_Col'] for col_map in COLUMN_MAP.values() 
                                     if col_map['E911_Report_Col'] != 'P:Project ID')) # Exclude P:Project ID
    
    df_report_subset = df_report[[col for col in df_report.columns if col in report_relevant_cols] + [MERGE_KEY_COL]].copy()
    
    # 5. Merge the DataFrames (Left Merge) (UNCHANGED)
    merged_df = pd.merge(
        df_daily_subset,
        df_report_subset,
        on=MERGE_KEY_COL, 
        how='left'
    )

    # 6. Discrepancy Finding Logic (UNCHANGED)
    # NOTE: The missing key check uses P:Project ID, which is now expected to be NaN after the merge 
    # if it truly doesn't exist in the NEW file. This is a robust way to find missing records.
    daily_comp_cols = [f"{col}_daily" for col in COMPARISON_COLS]
    report_comp_cols = [COLUMN_MAP[col]['E911_Report_Col'] for col in COMPARISON_COLS]
    
    daily_comp_cols_present = [col for col in daily_comp_cols if col in merged_df.columns]
    report_comp_cols_present = [col for col in report_comp_cols if col in merged_df.columns]

    merged_df['daily_concat'] = merged_df[daily_comp_cols_present].fillna('').astype(str).agg('::'.join, axis=1)
    
    # We must handle the case where P:Project ID is NOT present in the merged_df (since we excluded it from report_relevant_cols)
    # The columns P:Project ID will be NaN for all records in the merged_df if it wasn't present in the Excel file.
    
    # Safely get report comparison columns that actually exist in the merged_df
    report_cols_for_concat = [col for col in report_comp_cols if col in merged_df.columns]
    
    merged_df['report_concat'] = merged_df[report_cols_for_concat].fillna('').astype(str).agg('::'.join, axis=1)

    # The missing key check uses P:Project ID, which is now reliably NaN for all records if it was excluded from the subset.
    # This logic is now unreliable for finding missing records and should be adapted to check for the merge key
    
    # Check for two conditions: 1) Discrepancy in comparison fields OR 2) Missing Site in Report (NEW) file
    df_diff = merged_df[
        (merged_df['daily_concat'] != merged_df['report_concat']) |
        (merged_df[REPORT_KEY_HEADER].isna()) # Check if the NEW file's key is NaN (meaning record only exists in OLD)
    ].copy()
    
    print(f"Found {len(df_diff)} discrepancies.")

    # 7. Final Output Generation and Renaming (FIXED)
    if not df_diff.empty:
        
        output_source_map = {}
        for daily_col, map_details in COLUMN_MAP.items():
            output_col = map_details['Output_Col']
            report_col = map_details['E911_Report_Col']
            
            # --- Project ID FIX ---
            if daily_col == 'Project ID':
                # **CRITICAL**: Use the simple 'Project ID' header from the map, suffixed by '_daily'.
                old_suffixed_col = f"{daily_col}_daily" 
                output_source_map[old_suffixed_col] = output_col
                
            else:
                # All other columns, including the key, come from the NEW file.
                output_source_map[report_col] = output_col

        cols_to_select = [col for col in output_source_map.keys() if col in df_diff.columns]
        
        # Check if the mandatory Project ID column is in the merged data
        if f"{COLUMN_MAP['Project ID']['Output_Col']}_daily" not in df_diff.columns:
             print(f"FATAL ERROR: The required OLD Project ID column ('Project ID_daily') is missing after the merge. Please verify the header 'Project ID' in your CSV file.")
             return
        
        df_output = df_diff[cols_to_select].copy()
        
        # Apply the final rename map
        df_output.rename(columns=output_source_map, inplace=True)
        
        # Ensure the columns are in the correct final order
        df_output = df_output[output_cols_final_order]

        # Output File Generation
        timestamp = datetime.now().strftime('%m%d%Y_%H%M%S')
        output_file_name = f"QB_E911_{timestamp}.xlsx" 
        output_path = os.path.join(os.path.dirname(os.path.abspath(file_path_1)), output_file_name) 
        
        df_output.to_excel(output_path, index=False)
        print(f"âœ… Final New Data Report saved to: {output_path} (Project ID from OLD file, updates from NEW file)")
        
    else:
        print("âœ… No differences found. No report generated.")


